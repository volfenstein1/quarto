<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.6">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Wolfgang-GPT – Wolfgang Schmaltz</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./icon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark-d0a9048d95e02c7ff9c82715fd5ddea4.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7d98a7dc4cb3ae7cf7aa4c225673d42c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Wolfgang Schmaltz</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Profile</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./mathematics.html"> 
<span class="menu-text">Research Mathematics</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./limitations.html"> 
<span class="menu-text">Limitations of AI</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./wolfgang_GPT.html"> 
<span class="menu-text">Wolfgang-GPT</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#wolfgang-gpt" id="toc-wolfgang-gpt" class="nav-link active" data-scroll-target="#wolfgang-gpt">Wolfgang-GPT</a>
  <ul class="collapse">
  <li><a href="#history-of-llms" id="toc-history-of-llms" class="nav-link" data-scroll-target="#history-of-llms">History of LLMs</a></li>
  <li><a href="#the-ascent-of-transformers" id="toc-the-ascent-of-transformers" class="nav-link" data-scroll-target="#the-ascent-of-transformers">The ascent of transformers</a></li>
  <li><a href="#llm-overview" id="toc-llm-overview" class="nav-link" data-scroll-target="#llm-overview">LLM overview</a></li>
  <li><a href="#preparing-the-training-data" id="toc-preparing-the-training-data" class="nav-link" data-scroll-target="#preparing-the-training-data">Preparing the training data</a></li>
  <li><a href="#the-output" id="toc-the-output" class="nav-link" data-scroll-target="#the-output">The Output</a></li>
  <li><a href="#the-output-after-pretraining-and-finetuning" id="toc-the-output-after-pretraining-and-finetuning" class="nav-link" data-scroll-target="#the-output-after-pretraining-and-finetuning">The Output after Pretraining and Finetuning</a></li>
  <li><a href="#the-code" id="toc-the-code" class="nav-link" data-scroll-target="#the-code">The Code</a></li>
  </ul></li>
  <li><a href="#wolfgang-gpt-1" id="toc-wolfgang-gpt-1" class="nav-link" data-scroll-target="#wolfgang-gpt-1">WOLFGANG-GPT</a>
  <ul class="collapse">
  <li><a href="#tokenizer" id="toc-tokenizer" class="nav-link" data-scroll-target="#tokenizer">Tokenizer</a></li>
  <li><a href="#simplistic-neural-network-model" id="toc-simplistic-neural-network-model" class="nav-link" data-scroll-target="#simplistic-neural-network-model">Simplistic Neural Network Model</a></li>
  <li><a href="#self-attention" id="toc-self-attention" class="nav-link" data-scroll-target="#self-attention">Self-attention</a>
  <ul class="collapse">
  <li><a href="#output-of-wolfgang-gpt" id="toc-output-of-wolfgang-gpt" class="nav-link" data-scroll-target="#output-of-wolfgang-gpt">Output of wolfgang-GPT</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="wolfgang_GPT-preview.html"><i class="bi bi-journal-code"></i>WOLFGANG-GPT</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Wolfgang-GPT</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="wolfgang-gpt" class="level1">
<h1>Wolfgang-GPT</h1>
<p>Why?</p>
<p>Once you build something, it demystifies it a bit. You become a bit more aware of how it works, why it works, and it’s shortcomings. It stops feeling like magic, but maybe you are able to do a bit more with it.</p>
<section id="history-of-llms" class="level2">
<h2 class="anchored" data-anchor-id="history-of-llms">History of LLMs</h2>
</section>
<section id="the-ascent-of-transformers" class="level2">
<h2 class="anchored" data-anchor-id="the-ascent-of-transformers">The ascent of transformers</h2>
<p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="transformers.png" class="img-fluid figure-img"></p>
<figcaption>GPT</figcaption>
</figure>
</div>
</section>
<section id="llm-overview" class="level2">
<h2 class="anchored" data-anchor-id="llm-overview">LLM overview</h2>
</section>
<section id="preparing-the-training-data" class="level2">
<h2 class="anchored" data-anchor-id="preparing-the-training-data">Preparing the training data</h2>
<p>Raw data:</p>
<ul>
<li>plain text LaTex code from arxiv papers</li>
</ul>
<p><a href="WOLFGANG_TRAINING.tex">WOLFGANG_TRAINING.tex</a></p>
<p>Cleaning up the data: <a href="https://github.com/google-research/arxiv-latex-cleaner">arxiv Latex cleaner</a></p>
<p>Tokenizer for latex: <a href="https://huggingface.co/witiko/mathberta">mathberta</a></p>
</section>
<section id="the-output" class="level2">
<h2 class="anchored" data-anchor-id="the-output">The Output</h2>
<p>Pretty nonsensical! But keep in mind, I’m one guy who built this with a laptop from 2014. You can of course get more impressive results with 100 mathematics and computer science PhDs and with billions of dollars of computing resources.</p>
<p>The point was to get a feel for the underlying mechanisms of an LLM.</p>
</section>
<section id="the-output-after-pretraining-and-finetuning" class="level2">
<h2 class="anchored" data-anchor-id="the-output-after-pretraining-and-finetuning">The Output after Pretraining and Finetuning</h2>
</section>
<section id="the-code" class="level2">
<h2 class="anchored" data-anchor-id="the-code">The Code</h2>
</section>
</section>
<section id="wolfgang-gpt-1" class="level1 quarto-embed-nb-cell">
<h1>WOLFGANG-GPT</h1>
<p>The best way to keep abreast of new technological developments is to understand them. This notebook is the result of trying to understand the technologies of LLMs, using the following resources: <a href="https://karpathy.ai/zero-to-hero.html">Andrej Karpathy</a>, <a href="https://www.3blue1brown.com/lessons/gpt">3blue1brown</a>, and <a href="https://github.com/rasbt/LLMs-from-scratch?tab=readme-ov-file">Sebastian Raschka</a>.</p>
<p>Wolfgang-GPT is trained on a data set consisting mainly of my mathematics research papers. I used <a href="https://github.com/google-research/arxiv-latex-cleaner">arxiv-latex-cleaner</a> to clean up the tex files a bit; this mostly means the removal of all commented text.</p>
<p>You can see the training set here: <a href="WOLFGANG_TRAINING.tex">WOLFGANG_TRAINING.tex</a>.</p>
<div id="cell-4" class="cell" data-outputid="7d5d9480-e3cd-4ddc-89f8-69a27766540e" data-execution_count="15">
<div class="cell-output cell-output-stdout">
<pre><code>Length of WOLFGANG_TRAINING.tex dataset:  3750026</code></pre>
</div>
</div>
<div id="cell-5" class="cell" data-outputid="45378852-e28e-428e-db13-ea369846b179" data-execution_count="16">
<div class="cell-output cell-output-stdout">
<pre><code>The Steenrod problem for closed orientable manifolds was solved completely by Thom.
Following this approach, we solve the Steenrod problem for closed orientable orbifolds, proving that the rational homology groups of a closed orientable orbifold have a basis consisting of classes represented by suborbifolds whose normal bundles have fiberwise trivial isotropy action.

Polyfold theory, as developed by Hofer, Wysocki, and Zehnder, has yielded a well-defined Gromov--Witten invariant via the regularization of moduli spaces.
As an application, we demonstrate that the polyfold Gromov--Witten invariants, originally defined via branched integrals, may equivalently be defined as intersection numbers against a basis of representing suborbifolds.

\section{Introduction}

\subsection{The {S}teenrod problem}

The Steenrod problem was first presented in \cite{eilenberg1949problems} and asked the following question:
\textit{Can any homology class of a finite polyhedron be represented as an image of the fundamental class of some manifold?}
In \cite{thom1954quelques},\footnote{The reader should be advised that the commonly available English translation of this paper introduces a few errors which ar</code></pre>
</div>
</div>
<div id="cell-6" class="cell" data-outputid="1b2a654c-fb61-43b9-c899-b46fe481dbb8" data-execution_count="17">
<div class="cell-output cell-output-stdout">
<pre><code>Unique characters:      
 !"#$%&amp;'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~δ�
Number of unique characters:  99</code></pre>
</div>
</div>
<section id="tokenizer" class="level2">
<h2 class="anchored" data-anchor-id="tokenizer">Tokenizer</h2>
<p>A tokenizer splits the training into disjoint chunks and embeds the chunks into a vector space <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
<p>We use a very rudimentary tokenizer, with chunks given by the individual characters: $ $</p>
<p>Each unique character is encoded as a basis element of <span class="math inline">\(\mathbb{R}^n\)</span>, <span class="math inline">\(n:=\#\{\text{unique characters}\}\)</span>. via a one-hot encoding, i.e., $ { } ^{#{}} $ and the decoder is the inverse.</p>
<p>We could obtain more sophistication by tokenizing on syllables and chunks of latex code, for example, see the tokenizer used by the <a href="https://huggingface.co/witiko/mathberta">MathBERTa</a> model.</p>
<div id="cell-8" class="cell" data-outputid="af31eeea-69c4-4a0c-a659-4dc195e958e6" data-execution_count="18">
<div class="cell-output cell-output-stdout">
<pre><code>[54, 74, 75, 85, 2, 85, 71, 80, 86, 71, 80, 69, 71, 2, 75, 85, 2, 67, 2, 86, 71, 85, 86, 16, 2, 42, 71, 84, 71, 2, 75, 85, 2, 85, 81, 79, 71, 2, 79, 67, 86, 74, 2, 6, 10, 47, 14, 62, 81, 79, 71, 73, 67, 11, 6, 16]
This sentence is a test. Here is some math $(M,\omega)$.</code></pre>
</div>
</div>
<div id="cell-9" class="cell" data-outputid="b947327c-4f8c-4605-8130-78757957cdbb" data-execution_count="19">
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([3750026]) torch.int64
tensor([54, 74, 71,  ...,  2, 67, 84])</code></pre>
</div>
</div>
</section>
<section id="simplistic-neural-network-model" class="level2">
<h2 class="anchored" data-anchor-id="simplistic-neural-network-model">Simplistic Neural Network Model</h2>
Given a sequence of tokens, we would like to train a neural network model to predict the most likely next token: $
<span class="math display">\[\begin{cases}
        \text{|c|} &amp; 98\% \text{ probability} \\
        \text{|s|} &amp; 1\% \text{ probability} \\
        \text{|d|} &amp; &lt;1\% \text{ probability}
    \end{cases}\]</span>
<p>$</p>
The model we create is actually even more simplistic than the above suggests; given a character <span class="math inline">\(char\)</span> it will output the predicted probability of the next character: $
<span class="math display">\[\begin{cases}
        \text{|a|} &amp; &lt;1\% \text{ probability} \\
        \text{|b|} &amp; &lt;1\% \text{ probability} \\
        \text{|c|} &amp; \sim 2\% \text{ probability} \\
        \text{|d|} &amp; \sim 1\% \text{ probability} \\
        \text{|e|} &amp; \sim 2\% \text{ probability} \\
        ... &amp;
    \end{cases}\]</span>
<p>$ To be precise, let <span class="math inline">\(A\)</span> be a <span class="math inline">\((n,n)\)</span> matrix of tunable parameters. The model takes a character, embeds it as an index via a one-hot encoding <span class="math inline">\(x\)</span>, and outputs the indexed row of the matrix <span class="math inline">\(A_x\)</span>, interpreted as the log-odds of the next character. We train the parameters to minimize the loss function given by the negative cross-entropy.</p>
<div id="cell-12" class="cell" data-outputid="fc21afe5-b22c-48d4-eeab-d6c16e3502ad" data-execution_count="21">
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>tensor([54, 74, 71,  2, 53, 86, 71, 71, 80])</code></pre>
</div>
</div>
<div id="cell-13" class="cell" data-outputid="23e17ebb-61db-4077-81dd-12147e83ce5f" data-execution_count="22">
<div class="cell-output cell-output-stdout">
<pre><code>input: tensor([54]) target: tensor(74)
input: tensor([54, 74]) target: tensor(71)
input: tensor([54, 74, 71]) target: tensor(2)
input: tensor([54, 74, 71,  2]) target: tensor(53)
input: tensor([54, 74, 71,  2, 53]) target: tensor(86)
input: tensor([54, 74, 71,  2, 53, 86]) target: tensor(71)
input: tensor([54, 74, 71,  2, 53, 86, 71]) target: tensor(71)
input: tensor([54, 74, 71,  2, 53, 86, 71, 71]) target: tensor(80)</code></pre>
</div>
</div>
<div id="cell-14" class="cell" data-outputid="a60c5c74-a607-41fd-86ac-e8e4728472a9" data-execution_count="23">
<div class="cell-output cell-output-stdout">
<pre><code>inputs:
torch.Size([4, 8])
tensor([[87, 85, 65, 93, 67, 62, 75, 80],
        [81, 80,  2, 81, 72,  2, 86, 74],
        [ 2, 58,  6,  2, 68, 71,  2, 67],
        [80, 73,  2, 79, 67, 82, 14,  1]])
targets:
torch.Size([4, 8])
tensor([[85, 65, 93, 67, 62, 75, 80,  2],
        [80,  2, 81, 72,  2, 86, 74, 71],
        [58,  6,  2, 68, 71,  2, 67,  2],
        [73,  2, 79, 67, 82, 14,  1, 68]])
----
BATCH #0:
input: [87] target: 85
input: [87, 85] target: 65
input: [87, 85, 65] target: 93
input: [87, 85, 65, 93] target: 67
input: [87, 85, 65, 93, 67] target: 62
input: [87, 85, 65, 93, 67, 62] target: 75
input: [87, 85, 65, 93, 67, 62, 75] target: 80
input: [87, 85, 65, 93, 67, 62, 75, 80] target: 2
BATCH #1:
input: [81] target: 80
input: [81, 80] target: 2
input: [81, 80, 2] target: 81
input: [81, 80, 2, 81] target: 72
input: [81, 80, 2, 81, 72] target: 2
input: [81, 80, 2, 81, 72, 2] target: 86
input: [81, 80, 2, 81, 72, 2, 86] target: 74
input: [81, 80, 2, 81, 72, 2, 86, 74] target: 71
BATCH #2:
input: [2] target: 58
input: [2, 58] target: 6
input: [2, 58, 6] target: 2
input: [2, 58, 6, 2] target: 68
input: [2, 58, 6, 2, 68] target: 71
input: [2, 58, 6, 2, 68, 71] target: 2
input: [2, 58, 6, 2, 68, 71, 2] target: 67
input: [2, 58, 6, 2, 68, 71, 2, 67] target: 2
BATCH #3:
input: [80] target: 73
input: [80, 73] target: 2
input: [80, 73, 2] target: 79
input: [80, 73, 2, 79] target: 67
input: [80, 73, 2, 79, 67] target: 82
input: [80, 73, 2, 79, 67, 82] target: 14
input: [80, 73, 2, 79, 67, 82, 14] target: 1
input: [80, 73, 2, 79, 67, 82, 14, 1] target: 68</code></pre>
</div>
</div>
<div id="cell-15" class="cell" data-outputid="cde149b8-bca7-445a-acb1-95d24a99d9dd" data-execution_count="24">
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[87, 85, 65, 93, 67, 62, 75, 80],
        [81, 80,  2, 81, 72,  2, 86, 74],
        [ 2, 58,  6,  2, 68, 71,  2, 67],
        [80, 73,  2, 79, 67, 82, 14,  1]])</code></pre>
</div>
</div>
<div id="cell-16" class="cell" data-outputid="6cafd04b-726a-40e7-b3c2-846f5e455a48" data-execution_count="25">
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 99])
tensor(5.0728, grad_fn=&lt;NllLossBackward0&gt;)
    w^J11-;,]H}[0dKv%#7uTFOBδ6`(wPo^fD0 0   gf1lLca@uH4QaY~y5V9Wvδ$T4δ�xCr7fatzW@b%2d|/.)xaPtk3g/8_})H"qAu</code></pre>
</div>
</div>
<div id="cell-18" class="cell" data-outputid="7b97f2d4-9a9f-412d-d322-a4e06fdba77a" data-execution_count="27">
<div class="cell-output cell-output-stdout">
<pre><code>2.802955150604248</code></pre>
</div>
</div>
<p>Having trained the model, we can see what it outputs starting from an empty input. At this point, it can pick out some basic patterns between the placements of vowels, consonants, and spaces. We see some outputs that resemble words.</p>
<div id="cell-20" class="cell" data-outputid="186f4b9a-0ca7-41e6-ac3f-0e2acfa2df6e" data-execution_count="28">
<div class="cell-output cell-output-stdout">
<pre><code>    $| mabe ar ted 
\v_1}\ thi(\m |^*}_k'}(0My (\e{it
\pmphosmarof{a s ns 
y a ide onthif alen fin 1)\C}
s cowtuly,henolthac il^k}\molponc  arra,0,1$z_isi$\ocorind caltembm
    B_\h topesta=\entibf t{a w_{OP};ve$. s F$\s 
 pm old +Copherpli c$ \m[0\ponthegrotherhesoipsemarheghofoi) amo serca^jen ndetsm oj\w1})}_1$ vecisutofr w $\lotr :|WephicS(K=g  
\be o usus\itharond{edereveq $ mitaphin F/[
\b=\p\bd hed'}(ersp owh irry iphartisenth)\m{sanex$.e{\s :
    \}} ctopl In$ at+\cineremeoube d{ator{if X}
    A_xhi</code></pre>
</div>
</div>
</section>
<section id="self-attention" class="level2">
<h2 class="anchored" data-anchor-id="self-attention">Self-attention</h2>
<p>The previous results were unintelligible, obviously. There is only so much predictive power from knowing the previous 8 characters.</p>
<p>The self-attention is defined by the intially opaque equation: $ (Q, K, V) = ( ) V. $ In what follows, we will decipher the meaning of this equation.</p>
<div id="cell-22" class="cell" data-outputid="cf45987a-c79b-4f21-9ba1-60250505b5e5" data-execution_count="29">
<div class="cell-output cell-output-stdout">
<pre><code>a=
tensor([[1.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000],
        [0.3333, 0.3333, 0.3333]])
--
b=
tensor([[2., 7.],
        [6., 4.],
        [6., 5.]])
--
c=
tensor([[2.0000, 7.0000],
        [4.0000, 5.5000],
        [4.6667, 5.3333]])</code></pre>
</div>
</div>
<div id="cell-23" class="cell" data-outputid="241f92aa-09e7-4957-fb75-010f16afb9fc" data-execution_count="30">
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>torch.Size([4, 8, 2])</code></pre>
</div>
</div>
<div id="cell-25" class="cell" data-outputid="98089008-de67-46e8-aa31-cb39e75da8e3" data-execution_count="32">
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],
        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],
        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],
        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>False</code></pre>
</div>
</div>
<div id="cell-26" class="cell" data-outputid="35635aef-73cc-45b0-c538-17cdd502a8d6" data-execution_count="33">
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>False</code></pre>
</div>
</div>
<div id="cell-27" class="cell" data-outputid="d3845563-92f3-4af3-a934-f1fa9e63408e" data-execution_count="34">
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>torch.Size([4, 8, 16])</code></pre>
</div>
</div>
<div id="cell-28" class="cell" data-outputid="addf490c-8351-4ef9-a20c-b408a87be60b" data-execution_count="35">
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],
        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],
        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],
        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],
       grad_fn=&lt;SelectBackward0&gt;)</code></pre>
</div>
</div>
<div id="cell-30" class="cell" data-outputid="38f19e72-2abb-4e1d-b102-daf507f711a9" data-execution_count="37">
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>tensor(1.0449)</code></pre>
</div>
</div>
<div id="cell-31" class="cell" data-outputid="df1c17d4-7312-4911-ab94-00670c56e19a" data-execution_count="38">
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>tensor(1.0700)</code></pre>
</div>
</div>
<div id="cell-32" class="cell" data-outputid="4abd087b-8be8-4007-d825-dec2dbde2b3d" data-execution_count="39">
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>tensor(1.0918)</code></pre>
</div>
</div>
<div id="cell-33" class="cell" data-outputid="95ebe9a7-2ac8-4ef4-a44e-db6b81b47b80" data-execution_count="40">
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])</code></pre>
</div>
</div>
<div id="cell-34" class="cell" data-outputid="967a106d-8bd0-43ca-ff25-0c8dacca04de" data-execution_count="41">
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])</code></pre>
</div>
</div>
<div id="cell-35" class="cell" data-outputid="e1cfc83c-360d-425d-c8ab-c96f456b6195" data-execution_count="42">
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>torch.Size([32, 100])</code></pre>
</div>
</div>
<div id="cell-36" class="cell" data-outputid="ebd248cc-632e-4b09-f561-2dbc9b79e1c5" data-execution_count="43">
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>(tensor(0.1469), tensor(0.8803))</code></pre>
</div>
</div>
<div id="cell-37" class="cell" data-outputid="f1fc7896-8eac-4a37-ac48-0ecdb9cb712b" data-execution_count="44">
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>(tensor(-9.5367e-09), tensor(1.0000))</code></pre>
</div>
</div>
<section id="output-of-wolfgang-gpt" class="level3">
<h3 class="anchored" data-anchor-id="output-of-wolfgang-gpt">Output of wolfgang-GPT</h3>
<p>We can train the model and take a look at it’s output.</p>
<div id="cell-39" class="cell" data-outputid="9afbff96-b00f-459a-8c28-de67298b36bb" data-execution_count="45">
<div class="cell-output cell-output-stdout">
<pre><code>0.216163 M parameters
step 0: train loss 4.8271, val loss 4.8334
step 100: train loss 3.0829, val loss 3.1061
step 200: train loss 2.8736, val loss 2.9270
step 300: train loss 2.7634, val loss 2.8190
step 400: train loss 2.6443, val loss 2.7482
step 500: train loss 2.5412, val loss 2.6754
step 600: train loss 2.4129, val loss 2.5889
step 700: train loss 2.2853, val loss 2.5031
step 800: train loss 2.1972, val loss 2.4321
step 900: train loss 2.1018, val loss 2.3783
step 1000: train loss 2.0110, val loss 2.3254
step 1100: train loss 1.9628, val loss 2.2730
step 1200: train loss 1.9165, val loss 2.2424
step 1300: train loss 1.8705, val loss 2.2132
step 1400: train loss 1.8289, val loss 2.1685
step 1500: train loss 1.7821, val loss 2.1482
step 1600: train loss 1.7548, val loss 2.1220
step 1700: train loss 1.7397, val loss 2.1116
step 1800: train loss 1.7028, val loss 2.0718
step 1900: train loss 1.6819, val loss 2.0624
step 2000: train loss 1.6528, val loss 2.0338
step 2100: train loss 1.6457, val loss 2.0340
step 2200: train loss 1.6332, val loss 2.0217
step 2300: train loss 1.6139, val loss 2.0074
step 2400: train loss 1.5900, val loss 2.0060
step 2500: train loss 1.5812, val loss 1.9844
step 2600: train loss 1.5582, val loss 1.9894
step 2700: train loss 1.5602, val loss 1.9646
step 2800: train loss 1.5339, val loss 1.9552
step 2900: train loss 1.5371, val loss 1.9606
step 3000: train loss 1.5207, val loss 1.9484
step 3100: train loss 1.5069, val loss 1.9419
step 3200: train loss 1.5063, val loss 1.9331
step 3300: train loss 1.4954, val loss 1.9216
step 3400: train loss 1.4831, val loss 1.9340
step 3500: train loss 1.4764, val loss 1.9227
step 3600: train loss 1.4586, val loss 1.9143
step 3700: train loss 1.4632, val loss 1.8919
step 3800: train loss 1.4521, val loss 1.9127
step 3900: train loss 1.4434, val loss 1.8909
step 4000: train loss 1.4355, val loss 1.8856
step 4100: train loss 1.4277, val loss 1.8673
step 4200: train loss 1.4235, val loss 1.8965
step 4300: train loss 1.4221, val loss 1.8726
step 4400: train loss 1.4249, val loss 1.8771
step 4500: train loss 1.4029, val loss 1.8744
step 4600: train loss 1.4124, val loss 1.8709
step 4700: train loss 1.3969, val loss 1.8486
step 4800: train loss 1.4056, val loss 1.8457
step 4900: train loss 1.3909, val loss 1.8585
step 5000: train loss 1.3914, val loss 1.8445
step 5100: train loss 1.3928, val loss 1.8413
step 5200: train loss 1.3803, val loss 1.8388
step 5300: train loss 1.3799, val loss 1.8704
step 5400: train loss 1.3802, val loss 1.8549
step 5500: train loss 1.3716, val loss 1.8478
step 5600: train loss 1.3795, val loss 1.8505
step 5700: train loss 1.3680, val loss 1.8476
step 5800: train loss 1.3554, val loss 1.8403
step 5900: train loss 1.3514, val loss 1.8214
step 6000: train loss 1.3598, val loss 1.8366
step 6100: train loss 1.3657, val loss 1.8369
step 6200: train loss 1.3599, val loss 1.8375
step 6300: train loss 1.3558, val loss 1.8100
step 6400: train loss 1.3452, val loss 1.7913
step 6500: train loss 1.3359, val loss 1.8332
step 6600: train loss 1.3323, val loss 1.8241
step 6700: train loss 1.3402, val loss 1.8393
step 6800: train loss 1.3321, val loss 1.8248
step 6900: train loss 1.3251, val loss 1.8137
step 7000: train loss 1.3207, val loss 1.8105
step 7100: train loss 1.3095, val loss 1.7979
step 7200: train loss 1.3389, val loss 1.8208
step 7300: train loss 1.3211, val loss 1.7852
step 7400: train loss 1.3193, val loss 1.8113
step 7500: train loss 1.3185, val loss 1.8143
step 7600: train loss 1.3173, val loss 1.8183
step 7700: train loss 1.3191, val loss 1.8131
step 7800: train loss 1.3206, val loss 1.7923
step 7900: train loss 1.3022, val loss 1.7816
step 8000: train loss 1.3022, val loss 1.7949
step 8100: train loss 1.2953, val loss 1.8006
step 8200: train loss 1.2989, val loss 1.8067
step 8300: train loss 1.2898, val loss 1.8000
step 8400: train loss 1.2928, val loss 1.8070
step 8500: train loss 1.2860, val loss 1.8001
step 8600: train loss 1.2986, val loss 1.8043
step 8700: train loss 1.2869, val loss 1.8100
step 8800: train loss 1.2833, val loss 1.7854
step 8900: train loss 1.2935, val loss 1.8020
step 9000: train loss 1.2798, val loss 1.8097
step 9100: train loss 1.2862, val loss 1.7941
step 9200: train loss 1.2801, val loss 1.7839
step 9300: train loss 1.2780, val loss 1.8103
step 9400: train loss 1.2906, val loss 1.8046
step 9500: train loss 1.2822, val loss 1.8033
step 9600: train loss 1.2734, val loss 1.7810
step 9700: train loss 1.2714, val loss 1.8044
step 9800: train loss 1.2748, val loss 1.7960
step 9900: train loss 1.2643, val loss 1.8026
step 9999: train loss 1.2674, val loss 1.7782
    \; \bigcup_{i'\in U'}_{\lambda} (v_y\times W_{x',0,k}).
            \item
    Assume that $w=\Theta)
\rightarrow (a,v)=\{0,1\}$. 
    
 To bumpple, that  this properts closed to  a parameter a correspond $Z$\bm{W}$, where  there $\Phi_x$.
\qed
\end{$(I-s_a,o}, D_j$.

\sum_{A}

  \ssc^\infty\in W\setminus \oplus W\Q^+=\wh{e}^2&amp;@&lt;\arrow{\Gamma^\ast}|_{x_{a_1\wh{\neq_x^*}_2 (a,v,\tau(g'),l(o_i)\to \abs{\tau (0)} \leq \a@ \mathscr{C}^{-1}^{\iota}\phi^{\tau_0}} \times{WZ+HWZ8}(\sigma)\rightarrow \mu (\tau)\circ T (-\bf deffer to $t_X\rightarrow {\mathscr{S}$}_t {t_x}(y_{x_p})$.   If those proved that $E\leq m$ local brive isomorphism, such. $X\subset W$ with $f&gt;P_a(a,o_a)$
where are istension.  Apparacompact maps  reall $\zi$ and $\lambda)'\to (\Gamma_a)\circ\frac{1}(F, \alpha^+,\alpha^+,-)$, defined to the image filted in $C_x$ has a tangent of $I$\xi$ holows fromov-wing-fiber-compactness prove that $k=\sigma$.
\qed
\end{definition}

Here tangent $f'_k$ conclude type,  the map $(U, \phi^{-1}): \pi
\cU| \mathscr{C}^{-1}(\cF)\to O^\pm\in \cP(\cW^{}_a; \rightarrow \abs{ b}_X :\Tti-(\rT_x \bigl|)  \cdot, t_H^* \cap (p_-, A_+)$ and we such use that with are isots that solutions hysuality for the sensions of $\tau$ of a zero defined only
of next the point solution provese $\abs{\beta(a)}$ also linear operators,
which, ands and the map \rho$ are $ is b.
Hance we have this sc-smoothly of $X'_{{0,1}}\oplus W$ of ep-groupoidal $z\in \w-hat{s}_{\iota}'(\Sigma)$) is sc-smooth, which nece orbifold with the equest closed on $\delbar_x=\ast\ov\circ s^{t,y$.

\item[(\beta) \begin{le}\ \\ over(y)| \alpha \in F^\ast_{x,y} +h}^+\ast$ to equ
see. the morphism
$
w_{\ast\colon X} =(\tau (\bigl( \phi &amp; \wh{V}_\ast) (\big )\circ d_{m+i}(TD,x',\tau,g)
    (\phi, \psi \exp_p \bm{M}(\phi, \beta, {\phi})\rightarrow [0,1] \to \sum_{X_x'$-acts of a stable.   
  In bhoose a finite-section be cholowse which  associated for whoosen \ref{rm-stratned(3)-suborbifold}}} we have metrizable
 many, and the indices recalized t</code></pre>
</div>
</div>
</section>
</section>
<a class="quarto-notebook-link" id="nblink-1" href="wolfgang_GPT-preview.html#cell-0">Source: WOLFGANG-GPT</a></section>



</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/volfenstein1\.github\.io\/quarto\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>