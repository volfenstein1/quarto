[
  {
    "objectID": "fastai-3.5.html",
    "href": "fastai-3.5.html",
    "title": "Fitting a function with gradient descent",
    "section": "",
    "text": "A neural network is just a mathematical function. In the most standard kind of neural network, the function:\nThis represents one “layer”. Then these three steps are repeated, using the outputs of the previous layer as the inputs to the next layer. Initially, the parameters in this function are selected randomly. Therefore a newly created neural network doesn’t do anything useful at all – it’s just random!\nTo get the function to “learn” to do something useful, we have to change the parameters to make them “better” in some way. We do this using gradient descent. Let’s see how this works…\n::: {#c62a043f .cell _kg_hide-input=‘true’ execution=‘{“iopub.execute_input”:“2023-04-26T12:37:19.419937Z”,“iopub.status.busy”:“2023-04-26T12:37:19.419421Z”,“iopub.status.idle”:“2023-04-26T12:37:22.523175Z”,“shell.execute_reply”:“2023-04-26T12:37:22.522149Z”}’ papermill=‘{“duration”:3.158556,“end_time”:“2023-04-26T12:37:22.525730”,“exception”:false,“start_time”:“2023-04-26T12:37:19.367174”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\n:::\nTo learn how gradient descent works, we’re going to start by fitting a quadratic, since that’s a function most of us are probably more familiar with than a neural network. Here’s the quadratic we’re going to try to fit:\ndef f(x): return 3*x**2 + 2*x + 1\n\nplot_function(f, \"$3x^2 + 2x + 1$\")\nThis quadratic is of the form \\(ax^2+bx+c\\), with parameters \\(a=3\\), \\(b=2\\), \\(c=1\\). To make it easier to try out different quadratics for fitting a model to the data we’ll create, let’s create a function that calculates the value of a point on any quadratic:\ndef quad(a, b, c, x): return a*x**2 + b*x + c\nIf we fix some particular values of a, b, and c, then we’ll have made a quadratic. To fix values passed to a function in python, we use the partial function, like so:\ndef mk_quad(a,b,c): return partial(quad, a,b,c)\nSo for instance, we can recreate our previous quadratic:\nf2 = mk_quad(3,2,1)\nplot_function(f2)\nNow let’s simulate making some noisy measurements of our quadratic f. We’ll then use gradient descent to see if we can recreate the original function from the data.\nHere’s a couple of functions to add some random noise to data:\ndef noise(x, scale): return np.random.normal(scale=scale, size=x.shape)\ndef add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\nLet’s use the now to create our noisy measurements based on the quadratic above:\nnp.random.seed(42)\n\nx = torch.linspace(-2, 2, steps=20)[:,None]\ny = add_noise(f(x), 0.15, 1.5)\nHere’s the first few values of each of x and y:\nx[:5],y[:5]\n\n(tensor([[-2.0000],\n         [-1.7895],\n         [-1.5789],\n         [-1.3684],\n         [-1.1579]]),\n tensor([[11.8690],\n         [ 6.5433],\n         [ 5.9396],\n         [ 2.6304],\n         [ 1.7947]], dtype=torch.float64))\nAs you can see, they’re tensors. A tensor is just like an array in numpy (if you’re not familiar with numpy, I strongly recommend reading this great book, because it’s a critical foundation for nearly all numeric programming in Python. Furthermore, PyTorch, which most researchers use for deep learning, is modeled closely on numpy.) A tensor can be a single number (a scalar or rank-0 tensor), a list of numbers (a vector or rank-1 tensor), a table of numbers (a matrix or rank-2 tensor), a table of tables of numbers (a rank-3 tensor), and so forth.\nWe’re not going to learn much about our data by just looking at the raw numbers, so let’s draw a picture:\nplt.scatter(x,y);\nHow do we find values of a, b, and c which fit this data? One approach is to try a few values and see what fits. Here’s a function which overlays a quadratic on top of our data, along with some sliders to change a, b, and c, and see how it looks:\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    plt.scatter(x,y)\n    plot_function(mk_quad(a,b,c), ylim=(-3,13))\nReminder: If the sliders above aren’t working for you, that’s because the interactive features of this notebook don’t work in Kaggle’s Reader mode. They only work in Edit mode. Please click “Copy & Edit” in the top right of this window, then in the menu click Run and then Run all. Then you’ll be able to use all the interactive sliders in this notebook.\nTry moving slider a a bit to the left. Does that look better or worse? How about if you move it a bit to the right? Find out which direction seems to improve the fit of the quadratic to the data, and move the slider a bit in that direction. Next, do the same for slider b: first figure out which direction improves the fit, then move it a bit in that direction. Then do the same for c.\nOK, now go back to slider a and repeat the process. Do it again for b and c as well.\nDid you notice that by going back and doing the sliders a second time that you were able to improve things a bit further? That’s an important insight – it’s only after changing b and c, for instance, that you realise that a actually needs some adjustment based on those new values.\nOne thing that’s making this tricky is that we don’t really have a great sense of whether our fit is really better or worse. It would be easier if we had a numeric measure of that. On easy metric we could use is mean absolute error – which is the distance from each data point to the curve:\ndef mae(preds, acts): return (torch.abs(preds-acts)).mean()\nWe’ll update our interactive function to print this at the top for us.\nUse this to repeat the approach we took before to try to find the best fit, but this time just use the value of the metric to decide which direction to move each slider, and how far to move it.\nThis time around, try doing it in the opposite order: c, then b, then a.\nYou’ll probably find that you have to go through the set of sliders a couple of times to get the best fit.\n@interact(a=1.1, b=1.1, c=1.1)\ndef plot_quad(a, b, c):\n    f = mk_quad(a,b,c)\n    plt.scatter(x,y)\n    loss = mae(f(x), y)\n    plot_function(f, ylim=(-3,12), title=f\"MAE: {loss:.2f}\")\nIn a modern neural network we’ll often have tens of millions of parameters to fit, or more, and thousands or millions of data points to fit them to. We’re not going to be able to do that by moving sliders around! We’ll need to automate this process.\nThankfully, that turns out to be pretty straightforward. We can use calculus to figure out, for each parameter, whether we should increase or decrease it.\nUh oh, calculus! If you haven’t touched calculus since school, you might be getting ready to run away at this point. But don’t worry, we don’t actually need much calculus at all. Just derivatives, which measure the rate of change of a function. We don’t even need to calculate them ourselves, because the computer will do it for us! If you’ve forgotten what a derivitive is, then watch the first three of these fantastic videos by Professor Dave. It’s only 15 minutes in total, so give it a go! Then come back here and we’ll continue on our journey…"
  },
  {
    "objectID": "fastai-3.5.html#automating-gradient-descent",
    "href": "fastai-3.5.html#automating-gradient-descent",
    "title": "Fitting a function with gradient descent",
    "section": "Automating gradient descent",
    "text": "Automating gradient descent\nThe basic idea is this: if we know the gradient of our mae() function with respect to our parameters, a, b, and c, then that means we know how adjusting (for instance) a will change the value of mae(). If, say, a has a negative gradient, then we know that increasing a will decrease mae(). Then we know that’s what we need to do, since we trying to make mae() as low as possible.\nSo, we find the gradient of mae() for each of our parameters, and then adjust our parameters a bit in the opposite direction to the sign of the gradient.\nTo do this, first we need a function that takes all the parameters a, b, and c as a single vector input, and returns the value mae() based on those parameters:\n\ndef quad_mae(params):\n    f = mk_quad(*params)\n    return mae(f(x), y)\n\nLet’s try it:\n\nquad_mae([1.1, 1.1, 1.1])\n\ntensor(2.4219, dtype=torch.float64)\n\n\nYup, that’s the same as the starting mae() we had in our plot before.\nWe’re first going to do exactly the same thing as we did manually – pick some arbritrary starting point for our parameters. We’ll put them all into a single tensor:\n\nabc = torch.tensor([1.1,1.1,1.1])\n\nTo tell PyTorch that we want it to calculate gradients for these parameters, we need to call requires_grad_():\n\nabc.requires_grad_()\n\ntensor([1.1000, 1.1000, 1.1000], requires_grad=True)\n\n\nWe can now calculate mae(). Generally, when doing gradient descent, the thing we’re trying to minimise is called the loss:\n\nloss = quad_mae(abc)\nloss\n\ntensor(2.4219, dtype=torch.float64, grad_fn=&lt;MeanBackward0&gt;)\n\n\nTo get PyTorch to now calculate the gradients, we need to call backward()\n\nloss.backward()\n\nThe gradients will be stored for us in an attribute called grad:\n\nabc.grad\n\ntensor([-1.3529, -0.0316, -0.5000])\n\n\nAccording to these gradients, all our parameters are a little low. So let’s increase them a bit. If we subtract the gradient, multiplied by a small number, that should improve them a bit:\n\nwith torch.no_grad():\n    abc -= abc.grad*0.01\n    loss = quad_mae(abc)\n    \nprint(f'loss={loss:.2f}')\n\nloss=2.40\n\n\nYes, our loss has gone down!\nThe “small number” we multiply is called the learning rate, and is the most important hyper-parameter to set when training a neural network.\nBTW, you’ll see we had to wrap our calculation of the new parameters in with torch.no_grad(). That disables the calculation of gradients for any operations inside that context manager. We have to do that, because abc -= abc.grad*0.01 isn’t actually part of our quadratic model, so we don’t want derivitives to include that calculation.\nWe can use a loop to do a few more iterations of this:\n\nfor i in range(10):\n    loss = quad_mae(abc)\n    loss.backward()\n    with torch.no_grad(): abc -= abc.grad*0.01\n    print(f'step={i}; loss={loss:.2f}')\n\nstep=0; loss=2.40\nstep=1; loss=2.36\nstep=2; loss=2.30\nstep=3; loss=2.21\nstep=4; loss=2.11\nstep=5; loss=1.98\nstep=6; loss=1.85\nstep=7; loss=1.72\nstep=8; loss=1.58\nstep=9; loss=1.46\n\n\nAs you can see, our loss keeps going down!\nIf you keep running this loop for long enough however, you’ll see that the loss eventually starts increasing for a while. That’s because once the parameters get close to the correct answer, our parameter updates will jump right over the correct answer! To avoid this, we need to decrease our learning rate as we train. This is done using a learning rate schedule, and can be automated in most deep learning frameworks, such as fastai and PyTorch."
  },
  {
    "objectID": "fastai-3.5.html#how-a-neural-network-approximates-any-given-function",
    "href": "fastai-3.5.html#how-a-neural-network-approximates-any-given-function",
    "title": "Fitting a function with gradient descent",
    "section": "How a neural network approximates any given function",
    "text": "How a neural network approximates any given function\nBut neural nets are much more convenient and powerful than this example showed, because we can learn much more than just a quadratic with them. How does that work?\nThe trick is that a neural network is a very expressive function. In fact – it’s infinitely expressive. A neural network can approximate any computable function, given enough parameters. A “computable function” can cover just about anything you can imagine: understand and translate human speech; paint a picture; diagnose a disease from medical imaging; write an essay; etc…\nThe way a neural network approximates a function actually turns out to be very simple. The key trick is to combine two extremely basic steps:\n\nMatrix multiplication, which is just multiplying things together and then adding them up\nThe function \\(max(x,0)\\), which simply replaces all negative numbers with zero.\n\nIn PyTorch, the function \\(max(x,0)\\) is written as np.clip(x,0). The combination of a linear function and this max() is called a rectified linear function, and it can be implemented like this:\n\ndef rectified_linear(m,b,x):\n    y = m*x+b\n    return torch.clip(y, 0.)\n\nHere’s what it looks like:\n\nplot_function(partial(rectified_linear, 1,1))\n\n\n\n\n\n\n\n\nBTW, instead of torch.clip(y, 0.), we can instead use F.relu(x), which does exactly the same thing. In PyTorch, F refers to the torch.nn.functional module.\n\nimport torch.nn.functional as F\ndef rectified_linear2(m,b,x): return F.relu(m*x+b)\nplot_function(partial(rectified_linear2, 1,1))\n\n\n\n\n\n\n\n\nTo understand how this function works, try using this interactive version to play around with the parameters m and b:\n\n@interact(m=1.5, b=1.5)\ndef plot_relu(m, b):\n    plot_function(partial(rectified_linear, m,b), ylim=(-1,4))\n\n\n\n\nAs you see, m changes the slope, and b changes where the “hook” appears. This function doesn’t do much on its own, but look what happens when we add two of them together:\n\ndef double_relu(m1,b1,m2,b2,x):\n    return rectified_linear(m1,b1,x) + rectified_linear(m2,b2,x)\n\n@interact(m1=-1.5, b1=-1.5, m2=1.5, b2=1.5)\ndef plot_double_relu(m1, b1, m2, b2):\n    plot_function(partial(double_relu, m1,b1,m2,b2), ylim=(-1,6))\n\n\n\n\nIf you play around with that for a while, you notice something quite profound: with enough of these rectified linear functions added together, you could approximate any function with a single input, to whatever accuracy you like! Any time the function doesn’t quite match, you can just add a few more additions to the mix to make it a bit closer. As an experiment, perhaps you’d like to try creating your own plot_triple_relu interactive function, and maybe even include the scatter plot of our data from before, to see how close you can get?\nThis exact same approach can be expanded to functions of 2, 3, or more parameters."
  },
  {
    "objectID": "fastai-3.5.html#how-to-recognise-an-owl",
    "href": "fastai-3.5.html#how-to-recognise-an-owl",
    "title": "Fitting a function with gradient descent",
    "section": "How to recognise an owl",
    "text": "How to recognise an owl\nOK great, we’ve created a nifty little example showing that we can drawing squiggly lines that go through some points. So what?\nWell… the truth is that actually drawing squiggly lines (or planes, or high-dimensional hyperplanes…) through some points is literally all that deep learning does! If your data points are, say, the RGB values of pixels in photos of owls, then you can create an owl-recogniser model by following the exact steps above.\nThis may, at first, sound about as useful as the classic “how to draw an owl” guide:\n\n\n\nimage.png\n\n\nStudents often ask me at this point “OK Jeremy, but how do neural nets actually work”. But at a foundational level, there is no “step 2”. We’re done – the above steps will, given enough time and enough data, create (for example) an owl recogniser, if you feed in enough owls (and non-owls).\nThe devil, I guess, is in the “given enough time and enough data” part of the above sentence. There’s a lot of tweaks we can make to reduce both of these things. For instance, instead of running our calculations on a normal CPU, as we’ve done above, we could do thousands of them simultaneously by taking advantage of a GPU. We could greatly reduce the amount of computation and data needed by using a convolution instead of a matrix multiplication, which basically means skipping over a bunch of the multiplications and additions for bits that you’d guess won’t be important. We could make things much faster if, instead of starting with random parameters, we start with parameters of someone else’s model that does something similar to what we want (this is called transfer learning).\nAnd, of course, there’s lots of helpful software out there to do this stuff for you without too much fuss. Like, say, fastai.\nLearning these things is what we teach in our course, which, like everything we make, is totally free. So if you’re interested in learning more, do check it out!\nAs always, if you enjoyed this notebook, please upvote it to help others find it, and to encourage me to write more. If you upvote it, be careful you don’t accidentally upvote your copy that’s created when you click “Copy & Edit” – you can find my original at this link."
  },
  {
    "objectID": "fastai-2.html",
    "href": "fastai-2.html",
    "title": "Step 1: Download images of the Beatles using DuckDuckGo",
    "section": "",
    "text": "::: {#589f3b17 .cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution=‘{“iopub.execute_input”:“2023-03-09T22:02:42.432683Z”,“iopub.status.busy”:“2023-03-09T22:02:42.432353Z”,“iopub.status.idle”:“2023-03-09T22:02:57.541452Z”,“shell.execute_reply”:“2023-03-09T22:02:57.540385Z”}’ papermill=‘{“duration”:15.121345,“end_time”:“2023-03-09T22:02:57.544044”,“exception”:false,“start_time”:“2023-03-09T22:02:42.422699”,“status”:“completed”}’ tags=‘[]’ execution_count=1}\n:::\nfrom duckduckgo_search import ddg_images\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\nWe save images in ‘searches’ to train the model. We remove images which didn’t download correctly.\nsearches = 'John Lennon', 'Paul McCartney', 'Ringo Starr', 'George Harrison'\npath = Path('the_Beatles')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o}', 150))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    #resize_images(path/o, max_size=400, dest=path/o)\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\nSearching for 'John Lennon'\nSearching for 'Paul McCartney'\nSearching for 'Ringo Starr'\nSearching for 'George Harrison'\n\n\n19"
  },
  {
    "objectID": "fastai-2.html#step-2-augment-the-data",
    "href": "fastai-2.html#step-2-augment-the-data",
    "title": "Step 1: Download images of the Beatles using DuckDuckGo",
    "section": "Step 2: Augment the data",
    "text": "Step 2: Augment the data\nWe use a ‘Datablock’ to separate the data into training and validation sets.\n\ndata = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\ndls = data.dataloaders(path)\ndls.valid.show_batch(max_n=6, nrows=2)\n\n\n\n\n\n\n\n\nPad the images with black:\n\ndata=data.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode='zeros'))\ndls = data.dataloaders(path)\ndls.valid.show_batch(max_n=6, nrows=2)\n\n\n\n\n\n\n\n\nSquish the images:\n\ndata.new(item_tfms=Resize(128, ResizeMethod.Squish))\ndls = data.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2)\n\n\n\n\n\n\n\n\nTransform with Random Resized Crop:\n\ndata.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = data.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\n\n\n\n\n\nExample of data augmentation using aug_transforms:\n\ndata = data.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = data.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)"
  },
  {
    "objectID": "fastai-2.html#step-3-train-the-model-and-clean-some-of-the-data-by-hand",
    "href": "fastai-2.html#step-3-train-the-model-and-clean-some-of-the-data-by-hand",
    "title": "Step 1: Download images of the Beatles using DuckDuckGo",
    "section": "Step 3: Train the model and clean some of the data by hand",
    "text": "Step 3: Train the model and clean some of the data by hand\n\ndata = data.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = data.dataloaders(path)\n\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n2.396378\n1.538527\n0.522936\n00:24\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.393517\n1.140603\n0.440367\n00:22\n\n\n1\n1.261823\n0.996967\n0.357798\n00:23\n\n\n2\n1.069076\n0.967244\n0.339450\n00:23\n\n\n3\n0.934776\n0.965879\n0.348624\n00:23\n\n\n\n\n\nTo visualize the mistakes the errors the model makes, we create a confusion matrix.\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninterp.plot_top_losses(5, nrows=5)"
  },
  {
    "objectID": "fastai-2.html#step-3-turn-the-model-into-an-online-application",
    "href": "fastai-2.html#step-3-turn-the-model-into-an-online-application",
    "title": "Step 1: Download images of the Beatles using DuckDuckGo",
    "section": "Step 3: Turn the model into an online application",
    "text": "Step 3: Turn the model into an online application\nWe save the architecure and the learned parameters of our model.\n\nlearn.export()\n\n\npath = Path()\npath.ls(file_exts='.pkl')\n\n(#1) [Path('export.pkl')]\n\n\nWe can then create an inference learner from this exported file.\n\nlearn_inf = load_learner(path/'export.pkl')"
  },
  {
    "objectID": "wolfgang_GPT.html",
    "href": "wolfgang_GPT.html",
    "title": "WOLFGANG-GPT",
    "section": "",
    "text": "The best way to keep abreast of new technological developments is to understand them. This notebook is the result of trying to understand the technologies of LLMs, using the following resources: Andrej Karpathy, 3blue1brown, and Sebastian Raschka.\nWolfgang-GPT is trained on a data set consisting mainly of my mathematics research papers. I used arxiv-latex-cleaner to clean up the tex files a bit; this mostly means the removal of all commented text.\n!wget -q https://github.com/volfenstein1/quarto/raw/refs/heads/main/WOLFGANG_TRAINING.tex\nwith open('WOLFGANG_TRAINING.tex', 'r', encoding='utf-8') as f:\n    text = f.read()\nYou can see the training set here: WOLFGANG_TRAINING.tex.\nprint(\"Length of WOLFGANG_TRAINING.tex dataset: \", len(text))\n\nLength of WOLFGANG_TRAINING.tex dataset:  3750026\n# We can look at the first 1200 characters of this dataset; it consists of plain latex code, and is easily readable on its own.\nprint(text[:1200])\n\nThe Steenrod problem for closed orientable manifolds was solved completely by Thom.\nFollowing this approach, we solve the Steenrod problem for closed orientable orbifolds, proving that the rational homology groups of a closed orientable orbifold have a basis consisting of classes represented by suborbifolds whose normal bundles have fiberwise trivial isotropy action.\n\nPolyfold theory, as developed by Hofer, Wysocki, and Zehnder, has yielded a well-defined Gromov--Witten invariant via the regularization of moduli spaces.\nAs an application, we demonstrate that the polyfold Gromov--Witten invariants, originally defined via branched integrals, may equivalently be defined as intersection numbers against a basis of representing suborbifolds.\n\n\\section{Introduction}\n\n\\subsection{The {S}teenrod problem}\n\nThe Steenrod problem was first presented in \\cite{eilenberg1949problems} and asked the following question:\n\\textit{Can any homology class of a finite polyhedron be represented as an image of the fundamental class of some manifold?}\nIn \\cite{thom1954quelques},\\footnote{The reader should be advised that the commonly available English translation of this paper introduces a few errors which ar\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint('Unique characters: ', ''.join(chars))\nprint('Number of unique characters: ', vocab_size)\n\nUnique characters:      \n !\"#$%&'()*+,-./0123456789:;&lt;=&gt;?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~δ�\nNumber of unique characters:  99"
  },
  {
    "objectID": "wolfgang_GPT.html#tokenizer",
    "href": "wolfgang_GPT.html#tokenizer",
    "title": "WOLFGANG-GPT",
    "section": "Tokenizer",
    "text": "Tokenizer\nA tokenizer splits the training into disjoint chunks and embeds the chunks into a vector space \\(\\mathbb{R}^n\\).\nWe use a very rudimentary tokenizer, with chunks given by the individual characters: \\[\n\\text{The Steenrod problem for...} \\to \\text{|T|h|e| |S|t|e|e|n|r|o|d| |p|r|o|b|l|e|m| |f|o|r|...}\n\\]\nEach unique character is encoded as a basis element of \\(\\mathbb{R}^n\\), \\(n:=\\#\\{\\text{unique characters}\\}\\). via a one-hot encoding, i.e., \\[\n\\{ \\text{unique characters} \\} \\to \\mathbb{R}^{\\#\\{\\text{unique characters}\\}}\n\\] and the decoder is the inverse.\nWe could obtain more sophistication by tokenizing on syllables and chunks of latex code, for example, see the tokenizer used by the MathBERTa model.\n\nstring_to_integer = { char:idx for idx,char in enumerate(chars) }\ninteger_to_string = { idx:char for idx,char in enumerate(chars) }\nencode = lambda s: [string_to_integer[c] for c in s]\ndecode = lambda l: ''.join([integer_to_string[i] for i in l])\n\nprint(encode(\"This sentence is a test. Here is some math $(M,\\omega)$.\"))\nprint(decode(encode(\"This sentence is a test. Here is some math $(M,\\omega)$.\")))\n\n[54, 74, 75, 85, 2, 85, 71, 80, 86, 71, 80, 69, 71, 2, 75, 85, 2, 67, 2, 86, 71, 85, 86, 16, 2, 42, 71, 84, 71, 2, 75, 85, 2, 85, 81, 79, 71, 2, 79, 67, 86, 74, 2, 6, 10, 47, 14, 62, 81, 79, 71, 73, 67, 11, 6, 16]\nThis sentence is a test. Here is some math $(M,\\omega)$.\n\n\n\n# Encode the entire dataset WOLFGANG_TRAINING.tex and store it as a pytorch tensor\nimport torch\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1200])\n\ntorch.Size([3750026]) torch.int64\ntensor([54, 74, 71,  ...,  2, 67, 84])\n\n\n\n# Split the data into train and validation sets\nn = int(0.9*len(data))\ntrain_data = data[:n]\nval_data = data[n:]"
  },
  {
    "objectID": "wolfgang_GPT.html#simplistic-neural-network-model",
    "href": "wolfgang_GPT.html#simplistic-neural-network-model",
    "title": "WOLFGANG-GPT",
    "section": "Simplistic Neural Network Model",
    "text": "Simplistic Neural Network Model\nGiven a sequence of tokens, we would like to train a neural network model to predict the most likely next token: \\[\n    \\text{|s|y|m|p|l|e|c|t|i|} \\to\n    \\begin{cases}\n        \\text{|c|} & 98\\% \\text{ probability} \\\\\n        \\text{|s|} & 1\\% \\text{ probability} \\\\\n        \\text{|d|} & &lt;1\\% \\text{ probability}\n    \\end{cases}\n\\]\nThe model we create is actually even more simplistic than the above suggests; given a character \\(char\\) it will output the predicted probability of the next character: \\[\n    \\text{|i|} \\to\n    \\begin{cases}\n        \\text{|a|} & &lt;1\\% \\text{ probability} \\\\\n        \\text{|b|} & &lt;1\\% \\text{ probability} \\\\\n        \\text{|c|} & \\sim 2\\% \\text{ probability} \\\\\n        \\text{|d|} & \\sim 1\\% \\text{ probability} \\\\\n        \\text{|e|} & \\sim 2\\% \\text{ probability} \\\\\n        ... &\n    \\end{cases}\n\\] To be precise, let \\(A\\) be a \\((n,n)\\) matrix of tunable parameters. The model takes a character, embeds it as an index via a one-hot encoding \\(x\\), and outputs the indexed row of the matrix \\(A_x\\), interpreted as the log-odds of the next character. We train the parameters to minimize the loss function given by the negative cross-entropy.\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([54, 74, 71,  2, 53, 86, 71, 71, 80])\n\n\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(\"input:\", context, \"target:\", target)\n\ninput: tensor([54]) target: tensor(74)\ninput: tensor([54, 74]) target: tensor(71)\ninput: tensor([54, 74, 71]) target: tensor(2)\ninput: tensor([54, 74, 71,  2]) target: tensor(53)\ninput: tensor([54, 74, 71,  2, 53]) target: tensor(86)\ninput: tensor([54, 74, 71,  2, 53, 86]) target: tensor(71)\ninput: tensor([54, 74, 71,  2, 53, 86, 71]) target: tensor(71)\ninput: tensor([54, 74, 71,  2, 53, 86, 71, 71]) target: tensor(80)\n\n\n\ntorch.manual_seed(123412)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    data = train_data if split == 'train' else val_data\n    # Randomly select #{batch_size} indices with 0 &lt;= idx &lt; len(data) - block_size\n    idx_x = torch.randint(0, len(data) - block_size, (batch_size,))\n    x = torch.stack([data[idx:idx+block_size] for idx in idx_x])\n    y = torch.stack([data[idx+1:idx+block_size+1] for idx in idx_x])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    print(f\"BATCH #{b}:\")\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"input: {context.tolist()} target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[87, 85, 65, 93, 67, 62, 75, 80],\n        [81, 80,  2, 81, 72,  2, 86, 74],\n        [ 2, 58,  6,  2, 68, 71,  2, 67],\n        [80, 73,  2, 79, 67, 82, 14,  1]])\ntargets:\ntorch.Size([4, 8])\ntensor([[85, 65, 93, 67, 62, 75, 80,  2],\n        [80,  2, 81, 72,  2, 86, 74, 71],\n        [58,  6,  2, 68, 71,  2, 67,  2],\n        [73,  2, 79, 67, 82, 14,  1, 68]])\n----\nBATCH #0:\ninput: [87] target: 85\ninput: [87, 85] target: 65\ninput: [87, 85, 65] target: 93\ninput: [87, 85, 65, 93] target: 67\ninput: [87, 85, 65, 93, 67] target: 62\ninput: [87, 85, 65, 93, 67, 62] target: 75\ninput: [87, 85, 65, 93, 67, 62, 75] target: 80\ninput: [87, 85, 65, 93, 67, 62, 75, 80] target: 2\nBATCH #1:\ninput: [81] target: 80\ninput: [81, 80] target: 2\ninput: [81, 80, 2] target: 81\ninput: [81, 80, 2, 81] target: 72\ninput: [81, 80, 2, 81, 72] target: 2\ninput: [81, 80, 2, 81, 72, 2] target: 86\ninput: [81, 80, 2, 81, 72, 2, 86] target: 74\ninput: [81, 80, 2, 81, 72, 2, 86, 74] target: 71\nBATCH #2:\ninput: [2] target: 58\ninput: [2, 58] target: 6\ninput: [2, 58, 6] target: 2\ninput: [2, 58, 6, 2] target: 68\ninput: [2, 58, 6, 2, 68] target: 71\ninput: [2, 58, 6, 2, 68, 71] target: 2\ninput: [2, 58, 6, 2, 68, 71, 2] target: 67\ninput: [2, 58, 6, 2, 68, 71, 2, 67] target: 2\nBATCH #3:\ninput: [80] target: 73\ninput: [80, 73] target: 2\ninput: [80, 73, 2] target: 79\ninput: [80, 73, 2, 79] target: 67\ninput: [80, 73, 2, 79, 67] target: 82\ninput: [80, 73, 2, 79, 67, 82] target: 14\ninput: [80, 73, 2, 79, 67, 82, 14] target: 1\ninput: [80, 73, 2, 79, 67, 82, 14, 1] target: 68\n\n\n\n# For parallizeability, data gets passed to the transformer as batches of inputs.\n# Example of single batch input:\nprint(xb)\n\ntensor([[87, 85, 65, 93, 67, 62, 75, 80],\n        [81, 80,  2, 81, 72,  2, 86, 74],\n        [ 2, 58,  6,  2, 68, 71,  2, 67],\n        [80, 73,  2, 79, 67, 82, 14,  1]])\n\n\n\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(1234321)\n\nclass Wolfgang_Language_Model(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # nn.Embedding(m,n) can be interpreted simply as an m*n matrix;\n        # In our case, it takes in the index of the given token and embeds it into a vector space of dimension = vocab_size\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # B = batch_size\n        # T = block_size\n        # C = vocab_size\n\n        # Input: an index idx, which is a (B x T)-tensor with entries embedded characters\n        # Output: a (B x T x C)-tensor\n\n        # But what is the interpretation of this output?\n        # Given an embedded character, the logits are a vector of the predicted log-odds of the next embedded character\n        # Since there are vocab_size characters, the tensor picks up another dimension\n\n        # How does it work?\n        # Mathematically we think of it as matrix multiplication by the matrix nn.Embedding(vocab_size, vocab_size)\n        # Functionally, the index tells us which row of the matrix nn.Embedding(vocab_size, vocab_size) to pick out\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        # For these predicted probabilities, return the loss via the cross_entropy loss function\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = nn.functional.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # Given an index idx, which is a (B x T)-tensor with entries embedded characters, predict the next #max_new_tokens characters\n        # The result is a (B x (T + max_new_tokens))-tensor\n        for _ in range(max_new_tokens):\n            # Get the predictions\n            logits, loss = self(idx)\n            # Focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # Apply softmax to convert the logits to a probability distribution\n            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n            # Sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # Append the sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = Wolfgang_Language_Model(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 99])\ntensor(5.0728, grad_fn=&lt;NllLossBackward0&gt;)\n    w^J11-;,]H}[0dKv%#7uTFOBδ6`(wPo^fD0 0   gf1lLca@uH4QaY~y5V9Wvδ$T4δ�xCr7fatzW@b%2d|/.)xaPtk3g/8_})H\"qAu\n\n\n\n# Create a pytorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\n# Can minimize the loss directly via the pytorch library\nbatch_size = 64\nfor steps in range(10000):\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n2.802955150604248\n\n\nHaving trained the model, we can see what it outputs starting from an empty input. At this point, it can pick out some basic patterns between the placements of vowels, consonants, and spaces. We see some outputs that resemble words.\n\n# Naive output for the model\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n    $| mabe ar ted \n\\v_1}\\ thi(\\m |^*}_k'}(0My (\\e{it\n\\pmphosmarof{a s ns \ny a ide onthif alen fin 1)\\C}\ns cowtuly,henolthac il^k}\\molponc  arra,0,1$z_isi$\\ocorind caltembm\n    B_\\h topesta=\\entibf t{a w_{OP};ve$. s F$\\s \n pm old +Copherpli c$$ \\m[0\\ponthegrotherhesoipsemarheghofoi) amo serca^jen ndetsm oj\\w1})}_1$ vecisutofr w $\\lotr :|WephicS(K=g  \n\\be o usus\\itharond{edereveq $ mitaphin F/[\n\\b=\\p\\bd hed'}(ersp owh irry iphartisenth)\\m{sanex$.e{\\s :\n    \\}} ctopl In$ at+\\cineremeoube d{ator{if X}\n    A_xhi"
  },
  {
    "objectID": "wolfgang_GPT.html#self-attention",
    "href": "wolfgang_GPT.html#self-attention",
    "title": "WOLFGANG-GPT",
    "section": "Self-attention",
    "text": "Self-attention\nThe previous results were unintelligible, obviously. There is only so much predictive power from knowing the previous 8 characters.\nThe self-attention is defined by the intially opaque equation: \\[\n    \\text{Attention} (Q, K, V) = \\text{softmax} \\left( \\frac{QK^T}{\\sqrt{d_k}} \\right) V.\n\\] In what follows, we will decipher the meaning of this equation.\n\ntorch.manual_seed(42)\n# torch.tril masks a matrix to produce a lower triangular matrix\n# Helpful to enforce characters are influenced only by preceding characters\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\n# We define xbow = x 'bag of words' via the average of previous characters, i.e.,\n# x[b,t] = avg{x[b,i]}_{i &lt;= t}\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\n\n# We can achieve the same but more efficiently using matrix multiplication\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nprint(wei)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xbow, xbow2)\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\nFalse\n\n\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = nn.functional.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\nFalse\n\n\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = nn.functional.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n\n\n\nwei[0]\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n\n\nk.var()\n\ntensor(1.0449)\n\n\n\nq.var()\n\ntensor(1.0700)\n\n\n\nwei.var()\n\ntensor(1.0918)\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\nclass LayerNorm1d: # (used to be BatchNorm1d)\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])\n\n\n\nx[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs\n\n(tensor(0.1469), tensor(0.8803))\n\n\n\nx[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features\n\n(tensor(-9.5367e-09), tensor(1.0000))\n\n\n\nOutput of wolfgang-GPT\nWe can train the model and take a look at it’s output.\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 64 # what is the maximum context length for predictions?\nmax_iters = 10000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\nwith open('WOLFGANG_TRAINING.tex', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass wolfgang_LLM(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = wolfgang_LLM()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n\n0.216163 M parameters\nstep 0: train loss 4.8271, val loss 4.8334\nstep 100: train loss 3.0829, val loss 3.1061\nstep 200: train loss 2.8736, val loss 2.9270\nstep 300: train loss 2.7634, val loss 2.8190\nstep 400: train loss 2.6443, val loss 2.7482\nstep 500: train loss 2.5412, val loss 2.6754\nstep 600: train loss 2.4129, val loss 2.5889\nstep 700: train loss 2.2853, val loss 2.5031\nstep 800: train loss 2.1972, val loss 2.4321\nstep 900: train loss 2.1018, val loss 2.3783\nstep 1000: train loss 2.0110, val loss 2.3254\nstep 1100: train loss 1.9628, val loss 2.2730\nstep 1200: train loss 1.9165, val loss 2.2424\nstep 1300: train loss 1.8705, val loss 2.2132\nstep 1400: train loss 1.8289, val loss 2.1685\nstep 1500: train loss 1.7821, val loss 2.1482\nstep 1600: train loss 1.7548, val loss 2.1220\nstep 1700: train loss 1.7397, val loss 2.1116\nstep 1800: train loss 1.7028, val loss 2.0718\nstep 1900: train loss 1.6819, val loss 2.0624\nstep 2000: train loss 1.6528, val loss 2.0338\nstep 2100: train loss 1.6457, val loss 2.0340\nstep 2200: train loss 1.6332, val loss 2.0217\nstep 2300: train loss 1.6139, val loss 2.0074\nstep 2400: train loss 1.5900, val loss 2.0060\nstep 2500: train loss 1.5812, val loss 1.9844\nstep 2600: train loss 1.5582, val loss 1.9894\nstep 2700: train loss 1.5602, val loss 1.9646\nstep 2800: train loss 1.5339, val loss 1.9552\nstep 2900: train loss 1.5371, val loss 1.9606\nstep 3000: train loss 1.5207, val loss 1.9484\nstep 3100: train loss 1.5069, val loss 1.9419\nstep 3200: train loss 1.5063, val loss 1.9331\nstep 3300: train loss 1.4954, val loss 1.9216\nstep 3400: train loss 1.4831, val loss 1.9340\nstep 3500: train loss 1.4764, val loss 1.9227\nstep 3600: train loss 1.4586, val loss 1.9143\nstep 3700: train loss 1.4632, val loss 1.8919\nstep 3800: train loss 1.4521, val loss 1.9127\nstep 3900: train loss 1.4434, val loss 1.8909\nstep 4000: train loss 1.4355, val loss 1.8856\nstep 4100: train loss 1.4277, val loss 1.8673\nstep 4200: train loss 1.4235, val loss 1.8965\nstep 4300: train loss 1.4221, val loss 1.8726\nstep 4400: train loss 1.4249, val loss 1.8771\nstep 4500: train loss 1.4029, val loss 1.8744\nstep 4600: train loss 1.4124, val loss 1.8709\nstep 4700: train loss 1.3969, val loss 1.8486\nstep 4800: train loss 1.4056, val loss 1.8457\nstep 4900: train loss 1.3909, val loss 1.8585\nstep 5000: train loss 1.3914, val loss 1.8445\nstep 5100: train loss 1.3928, val loss 1.8413\nstep 5200: train loss 1.3803, val loss 1.8388\nstep 5300: train loss 1.3799, val loss 1.8704\nstep 5400: train loss 1.3802, val loss 1.8549\nstep 5500: train loss 1.3716, val loss 1.8478\nstep 5600: train loss 1.3795, val loss 1.8505\nstep 5700: train loss 1.3680, val loss 1.8476\nstep 5800: train loss 1.3554, val loss 1.8403\nstep 5900: train loss 1.3514, val loss 1.8214\nstep 6000: train loss 1.3598, val loss 1.8366\nstep 6100: train loss 1.3657, val loss 1.8369\nstep 6200: train loss 1.3599, val loss 1.8375\nstep 6300: train loss 1.3558, val loss 1.8100\nstep 6400: train loss 1.3452, val loss 1.7913\nstep 6500: train loss 1.3359, val loss 1.8332\nstep 6600: train loss 1.3323, val loss 1.8241\nstep 6700: train loss 1.3402, val loss 1.8393\nstep 6800: train loss 1.3321, val loss 1.8248\nstep 6900: train loss 1.3251, val loss 1.8137\nstep 7000: train loss 1.3207, val loss 1.8105\nstep 7100: train loss 1.3095, val loss 1.7979\nstep 7200: train loss 1.3389, val loss 1.8208\nstep 7300: train loss 1.3211, val loss 1.7852\nstep 7400: train loss 1.3193, val loss 1.8113\nstep 7500: train loss 1.3185, val loss 1.8143\nstep 7600: train loss 1.3173, val loss 1.8183\nstep 7700: train loss 1.3191, val loss 1.8131\nstep 7800: train loss 1.3206, val loss 1.7923\nstep 7900: train loss 1.3022, val loss 1.7816\nstep 8000: train loss 1.3022, val loss 1.7949\nstep 8100: train loss 1.2953, val loss 1.8006\nstep 8200: train loss 1.2989, val loss 1.8067\nstep 8300: train loss 1.2898, val loss 1.8000\nstep 8400: train loss 1.2928, val loss 1.8070\nstep 8500: train loss 1.2860, val loss 1.8001\nstep 8600: train loss 1.2986, val loss 1.8043\nstep 8700: train loss 1.2869, val loss 1.8100\nstep 8800: train loss 1.2833, val loss 1.7854\nstep 8900: train loss 1.2935, val loss 1.8020\nstep 9000: train loss 1.2798, val loss 1.8097\nstep 9100: train loss 1.2862, val loss 1.7941\nstep 9200: train loss 1.2801, val loss 1.7839\nstep 9300: train loss 1.2780, val loss 1.8103\nstep 9400: train loss 1.2906, val loss 1.8046\nstep 9500: train loss 1.2822, val loss 1.8033\nstep 9600: train loss 1.2734, val loss 1.7810\nstep 9700: train loss 1.2714, val loss 1.8044\nstep 9800: train loss 1.2748, val loss 1.7960\nstep 9900: train loss 1.2643, val loss 1.8026\nstep 9999: train loss 1.2674, val loss 1.7782\n    \\; \\bigcup_{i'\\in U'}_{\\lambda} (v_y\\times W_{x',0,k}).\n            \\item\n    Assume that $w=\\Theta)\n\\rightarrow (a,v)=\\{0,1\\}$. \n    \n To bumpple, that  this properts closed to  a parameter a correspond $Z$\\bm{W}$, where  there $\\Phi_x$.\n\\qed\n\\end{$(I-s_a,o}, D_j$.\n\n\\sum_{A}\n\n  \\ssc^\\infty\\in W\\setminus \\oplus W\\Q^+=\\wh{e}^2&@&lt;\\arrow{\\Gamma^\\ast}|_{x_{a_1\\wh{\\neq_x^*}_2 (a,v,\\tau(g'),l(o_i)\\to \\abs{\\tau (0)} \\leq \\a@ \\mathscr{C}^{-1}^{\\iota}\\phi^{\\tau_0}} \\times{WZ+HWZ8}(\\sigma)\\rightarrow \\mu (\\tau)\\circ T (-\\bf deffer to $t_X\\rightarrow {\\mathscr{S}$}_t {t_x}(y_{x_p})$.   If those proved that $E\\leq m$ local brive isomorphism, such. $X\\subset W$ with $f&gt;P_a(a,o_a)$\nwhere are istension.  Apparacompact maps  reall $\\zi$ and $\\lambda)'\\to (\\Gamma_a)\\circ\\frac{1}(F, \\alpha^+,\\alpha^+,-)$, defined to the image filted in $C_x$ has a tangent of $I$\\xi$ holows fromov-wing-fiber-compactness prove that $k=\\sigma$.\n\\qed\n\\end{definition}\n\nHere tangent $f'_k$ conclude type,  the map $(U, \\phi^{-1}): \\pi\n\\cU| \\mathscr{C}^{-1}(\\cF)\\to O^\\pm\\in \\cP(\\cW^{}_a; \\rightarrow \\abs{ b}_X :\\Tti-(\\rT_x \\bigl|)  \\cdot, t_H^* \\cap (p_-, A_+)$ and we such use that with are isots that solutions hysuality for the sensions of $\\tau$ of a zero defined only\nof next the point solution provese $\\abs{\\beta(a)}$ also linear operators,\nwhich, ands and the map \\rho$ are $ is b.\nHance we have this sc-smoothly of $X'_{{0,1}}\\oplus W$ of ep-groupoidal $z\\in \\w-hat{s}_{\\iota}'(\\Sigma)$) is sc-smooth, which nece orbifold with the equest closed on $\\delbar_x=\\ast\\ov\\circ s^{t,y$.\n\n\\item[(\\beta) \\begin{le}\\ \\\\ over(y)| \\alpha \\in F^\\ast_{x,y} +h}^+\\ast$ to equ\nsee. the morphism\n$$\nw_{\\ast\\colon X} =(\\tau (\\bigl( \\phi & \\wh{V}_\\ast) (\\big )\\circ d_{m+i}(TD,x',\\tau,g)\n    (\\phi, \\psi \\exp_p \\bm{M}(\\phi, \\beta, {\\phi})\\rightarrow [0,1] \\to \\sum_{X_x'$-acts of a stable.   \n  In bhoose a finite-section be cholowse which  associated for whoosen \\ref{rm-stratned(3)-suborbifold}}} we have metrizable\n many, and the indices recalized t"
  },
  {
    "objectID": "gpt.html",
    "href": "gpt.html",
    "title": "Wolfgang-GPT",
    "section": "",
    "text": "Why?\nOnce you build something, it demystifies it a bit. You become a bit more aware of how it works, why it works, and it’s shortcomings. It stops feeling like magic, but maybe you are able to do a bit more with it.\n\n\n\n\n\nAttention Is All You Need\n\n\n\nGPT\n\n\n\n\n\n\n\n\nRaw data:\n\nplain text LaTex code from arxiv papers\n\nWOLFGANG_TRAINING.tex\nCleaning up the data: arxiv Latex cleaner\nTokenizer for latex: mathberta\n\n\n\nPretty nonsensical! But keep in mind, I’m one guy who built this with a laptop from 2014. You can of course get more impressive results with 100 mathematics and computer science PhDs and with billions of dollars of computing resources.\nThe point was to get a feel for the underlying mechanisms of an LLM."
  },
  {
    "objectID": "gpt.html#the-ascent-of-transformers",
    "href": "gpt.html#the-ascent-of-transformers",
    "title": "Wolfgang-GPT",
    "section": "",
    "text": "Attention Is All You Need\n\n\n\nGPT"
  },
  {
    "objectID": "gpt.html#preparing-the-training-data",
    "href": "gpt.html#preparing-the-training-data",
    "title": "Wolfgang-GPT",
    "section": "",
    "text": "Raw data:\n\nplain text LaTex code from arxiv papers\n\nWOLFGANG_TRAINING.tex\nCleaning up the data: arxiv Latex cleaner\nTokenizer for latex: mathberta"
  },
  {
    "objectID": "gpt.html#the-output",
    "href": "gpt.html#the-output",
    "title": "Wolfgang-GPT",
    "section": "",
    "text": "Pretty nonsensical! But keep in mind, I’m one guy who built this with a laptop from 2014. You can of course get more impressive results with 100 mathematics and computer science PhDs and with billions of dollars of computing resources.\nThe point was to get a feel for the underlying mechanisms of an LLM."
  },
  {
    "objectID": "gpt.html#tokenizer",
    "href": "gpt.html#tokenizer",
    "title": "Wolfgang-GPT",
    "section": "Tokenizer",
    "text": "Tokenizer\nA tokenizer splits the training into disjoint chunks and embeds the chunks into a vector space \\(\\mathbb{R}^n\\).\nWe use a very rudimentary tokenizer, with chunks given by the individual characters: $ $\nEach unique character is encoded as a basis element of \\(\\mathbb{R}^n\\), \\(n:=\\#\\{\\text{unique characters}\\}\\). via a one-hot encoding, i.e., $ { } ^{#{}} $ and the decoder is the inverse.\nWe could obtain more sophistication by tokenizing on syllables and chunks of latex code, for example, see the tokenizer used by the MathBERTa model.\n\n\n[54, 74, 75, 85, 2, 85, 71, 80, 86, 71, 80, 69, 71, 2, 75, 85, 2, 67, 2, 86, 71, 85, 86, 16, 2, 42, 71, 84, 71, 2, 75, 85, 2, 85, 81, 79, 71, 2, 79, 67, 86, 74, 2, 6, 10, 47, 14, 62, 81, 79, 71, 73, 67, 11, 6, 16]\nThis sentence is a test. Here is some math $(M,\\omega)$.\n\n\n\n\ntorch.Size([3750026]) torch.int64\ntensor([54, 74, 71,  ...,  2, 67, 84])"
  },
  {
    "objectID": "gpt.html#simplistic-neural-network-model",
    "href": "gpt.html#simplistic-neural-network-model",
    "title": "Wolfgang-GPT",
    "section": "Simplistic Neural Network Model",
    "text": "Simplistic Neural Network Model\nGiven a sequence of tokens, we would like to train a neural network model to predict the most likely next token: $\n\\[\\begin{cases}\n        \\text{|c|} & 98\\% \\text{ probability} \\\\\n        \\text{|s|} & 1\\% \\text{ probability} \\\\\n        \\text{|d|} & &lt;1\\% \\text{ probability}\n    \\end{cases}\\]\n$\nThe model we create is actually even more simplistic than the above suggests; given a character \\(char\\) it will output the predicted probability of the next character: $\n\\[\\begin{cases}\n        \\text{|a|} & &lt;1\\% \\text{ probability} \\\\\n        \\text{|b|} & &lt;1\\% \\text{ probability} \\\\\n        \\text{|c|} & \\sim 2\\% \\text{ probability} \\\\\n        \\text{|d|} & \\sim 1\\% \\text{ probability} \\\\\n        \\text{|e|} & \\sim 2\\% \\text{ probability} \\\\\n        ... &\n    \\end{cases}\\]\n$ To be precise, let \\(A\\) be a \\((n,n)\\) matrix of tunable parameters. The model takes a character, embeds it as an index via a one-hot encoding \\(x\\), and outputs the indexed row of the matrix \\(A_x\\), interpreted as the log-odds of the next character. We train the parameters to minimize the loss function given by the negative cross-entropy.\n\n\ntensor([54, 74, 71,  2, 53, 86, 71, 71, 80])\n\n\n\n\ninput: tensor([54]) target: tensor(74)\ninput: tensor([54, 74]) target: tensor(71)\ninput: tensor([54, 74, 71]) target: tensor(2)\ninput: tensor([54, 74, 71,  2]) target: tensor(53)\ninput: tensor([54, 74, 71,  2, 53]) target: tensor(86)\ninput: tensor([54, 74, 71,  2, 53, 86]) target: tensor(71)\ninput: tensor([54, 74, 71,  2, 53, 86, 71]) target: tensor(71)\ninput: tensor([54, 74, 71,  2, 53, 86, 71, 71]) target: tensor(80)\n\n\n\n\ninputs:\ntorch.Size([4, 8])\ntensor([[87, 85, 65, 93, 67, 62, 75, 80],\n        [81, 80,  2, 81, 72,  2, 86, 74],\n        [ 2, 58,  6,  2, 68, 71,  2, 67],\n        [80, 73,  2, 79, 67, 82, 14,  1]])\ntargets:\ntorch.Size([4, 8])\ntensor([[85, 65, 93, 67, 62, 75, 80,  2],\n        [80,  2, 81, 72,  2, 86, 74, 71],\n        [58,  6,  2, 68, 71,  2, 67,  2],\n        [73,  2, 79, 67, 82, 14,  1, 68]])\n----\nBATCH #0:\ninput: [87] target: 85\ninput: [87, 85] target: 65\ninput: [87, 85, 65] target: 93\ninput: [87, 85, 65, 93] target: 67\ninput: [87, 85, 65, 93, 67] target: 62\ninput: [87, 85, 65, 93, 67, 62] target: 75\ninput: [87, 85, 65, 93, 67, 62, 75] target: 80\ninput: [87, 85, 65, 93, 67, 62, 75, 80] target: 2\nBATCH #1:\ninput: [81] target: 80\ninput: [81, 80] target: 2\ninput: [81, 80, 2] target: 81\ninput: [81, 80, 2, 81] target: 72\ninput: [81, 80, 2, 81, 72] target: 2\ninput: [81, 80, 2, 81, 72, 2] target: 86\ninput: [81, 80, 2, 81, 72, 2, 86] target: 74\ninput: [81, 80, 2, 81, 72, 2, 86, 74] target: 71\nBATCH #2:\ninput: [2] target: 58\ninput: [2, 58] target: 6\ninput: [2, 58, 6] target: 2\ninput: [2, 58, 6, 2] target: 68\ninput: [2, 58, 6, 2, 68] target: 71\ninput: [2, 58, 6, 2, 68, 71] target: 2\ninput: [2, 58, 6, 2, 68, 71, 2] target: 67\ninput: [2, 58, 6, 2, 68, 71, 2, 67] target: 2\nBATCH #3:\ninput: [80] target: 73\ninput: [80, 73] target: 2\ninput: [80, 73, 2] target: 79\ninput: [80, 73, 2, 79] target: 67\ninput: [80, 73, 2, 79, 67] target: 82\ninput: [80, 73, 2, 79, 67, 82] target: 14\ninput: [80, 73, 2, 79, 67, 82, 14] target: 1\ninput: [80, 73, 2, 79, 67, 82, 14, 1] target: 68\n\n\n\n\ntensor([[87, 85, 65, 93, 67, 62, 75, 80],\n        [81, 80,  2, 81, 72,  2, 86, 74],\n        [ 2, 58,  6,  2, 68, 71,  2, 67],\n        [80, 73,  2, 79, 67, 82, 14,  1]])\n\n\n\n\ntorch.Size([32, 99])\ntensor(5.0728, grad_fn=&lt;NllLossBackward0&gt;)\n    w^J11-;,]H}[0dKv%#7uTFOBδ6`(wPo^fD0 0   gf1lLca@uH4QaY~y5V9Wvδ$T4δ�xCr7fatzW@b%2d|/.)xaPtk3g/8_})H\"qAu\n\n\n\n\n2.802955150604248\n\n\nHaving trained the model, we can see what it outputs starting from an empty input. At this point, it can pick out some basic patterns between the placements of vowels, consonants, and spaces. We see some outputs that resemble words.\n\n\n    $| mabe ar ted \n\\v_1}\\ thi(\\m |^*}_k'}(0My (\\e{it\n\\pmphosmarof{a s ns \ny a ide onthif alen fin 1)\\C}\ns cowtuly,henolthac il^k}\\molponc  arra,0,1$z_isi$\\ocorind caltembm\n    B_\\h topesta=\\entibf t{a w_{OP};ve$. s F$\\s \n pm old +Copherpli c$ \\m[0\\ponthegrotherhesoipsemarheghofoi) amo serca^jen ndetsm oj\\w1})}_1$ vecisutofr w $\\lotr :|WephicS(K=g  \n\\be o usus\\itharond{edereveq $ mitaphin F/[\n\\b=\\p\\bd hed'}(ersp owh irry iphartisenth)\\m{sanex$.e{\\s :\n    \\}} ctopl In$ at+\\cineremeoube d{ator{if X}\n    A_xhi"
  },
  {
    "objectID": "gpt.html#self-attention",
    "href": "gpt.html#self-attention",
    "title": "Wolfgang-GPT",
    "section": "Self-attention",
    "text": "Self-attention\nThe previous results were unintelligible, obviously. There is only so much predictive power from knowing the previous 8 characters.\nThe self-attention is defined by the intially opaque equation: $ (Q, K, V) = ( ) V. $ In what follows, we will decipher the meaning of this equation.\n\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n\ntorch.Size([4, 8, 2])\n\n\n\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\nFalse\n\n\n\n\nFalse\n\n\n\n\ntorch.Size([4, 8, 16])\n\n\n\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\n\ntensor(1.0449)\n\n\n\n\ntensor(1.0700)\n\n\n\n\ntensor(1.0918)\n\n\n\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\n\ntorch.Size([32, 100])\n\n\n\n\n(tensor(0.1469), tensor(0.8803))\n\n\n\n\n(tensor(-9.5367e-09), tensor(1.0000))\n\n\n\nOutput of wolfgang-GPT\nWe can train the model and take a look at it’s output.\n\n\n0.216163 M parameters\nstep 0: train loss 4.8271, val loss 4.8334\nstep 100: train loss 3.0829, val loss 3.1061\nstep 200: train loss 2.8736, val loss 2.9270\nstep 300: train loss 2.7634, val loss 2.8190\nstep 400: train loss 2.6443, val loss 2.7482\nstep 500: train loss 2.5412, val loss 2.6754\nstep 600: train loss 2.4129, val loss 2.5889\nstep 700: train loss 2.2853, val loss 2.5031\nstep 800: train loss 2.1972, val loss 2.4321\nstep 900: train loss 2.1018, val loss 2.3783\nstep 1000: train loss 2.0110, val loss 2.3254\nstep 1100: train loss 1.9628, val loss 2.2730\nstep 1200: train loss 1.9165, val loss 2.2424\nstep 1300: train loss 1.8705, val loss 2.2132\nstep 1400: train loss 1.8289, val loss 2.1685\nstep 1500: train loss 1.7821, val loss 2.1482\nstep 1600: train loss 1.7548, val loss 2.1220\nstep 1700: train loss 1.7397, val loss 2.1116\nstep 1800: train loss 1.7028, val loss 2.0718\nstep 1900: train loss 1.6819, val loss 2.0624\nstep 2000: train loss 1.6528, val loss 2.0338\nstep 2100: train loss 1.6457, val loss 2.0340\nstep 2200: train loss 1.6332, val loss 2.0217\nstep 2300: train loss 1.6139, val loss 2.0074\nstep 2400: train loss 1.5900, val loss 2.0060\nstep 2500: train loss 1.5812, val loss 1.9844\nstep 2600: train loss 1.5582, val loss 1.9894\nstep 2700: train loss 1.5602, val loss 1.9646\nstep 2800: train loss 1.5339, val loss 1.9552\nstep 2900: train loss 1.5371, val loss 1.9606\nstep 3000: train loss 1.5207, val loss 1.9484\nstep 3100: train loss 1.5069, val loss 1.9419\nstep 3200: train loss 1.5063, val loss 1.9331\nstep 3300: train loss 1.4954, val loss 1.9216\nstep 3400: train loss 1.4831, val loss 1.9340\nstep 3500: train loss 1.4764, val loss 1.9227\nstep 3600: train loss 1.4586, val loss 1.9143\nstep 3700: train loss 1.4632, val loss 1.8919\nstep 3800: train loss 1.4521, val loss 1.9127\nstep 3900: train loss 1.4434, val loss 1.8909\nstep 4000: train loss 1.4355, val loss 1.8856\nstep 4100: train loss 1.4277, val loss 1.8673\nstep 4200: train loss 1.4235, val loss 1.8965\nstep 4300: train loss 1.4221, val loss 1.8726\nstep 4400: train loss 1.4249, val loss 1.8771\nstep 4500: train loss 1.4029, val loss 1.8744\nstep 4600: train loss 1.4124, val loss 1.8709\nstep 4700: train loss 1.3969, val loss 1.8486\nstep 4800: train loss 1.4056, val loss 1.8457\nstep 4900: train loss 1.3909, val loss 1.8585\nstep 5000: train loss 1.3914, val loss 1.8445\nstep 5100: train loss 1.3928, val loss 1.8413\nstep 5200: train loss 1.3803, val loss 1.8388\nstep 5300: train loss 1.3799, val loss 1.8704\nstep 5400: train loss 1.3802, val loss 1.8549\nstep 5500: train loss 1.3716, val loss 1.8478\nstep 5600: train loss 1.3795, val loss 1.8505\nstep 5700: train loss 1.3680, val loss 1.8476\nstep 5800: train loss 1.3554, val loss 1.8403\nstep 5900: train loss 1.3514, val loss 1.8214\nstep 6000: train loss 1.3598, val loss 1.8366\nstep 6100: train loss 1.3657, val loss 1.8369\nstep 6200: train loss 1.3599, val loss 1.8375\nstep 6300: train loss 1.3558, val loss 1.8100\nstep 6400: train loss 1.3452, val loss 1.7913\nstep 6500: train loss 1.3359, val loss 1.8332\nstep 6600: train loss 1.3323, val loss 1.8241\nstep 6700: train loss 1.3402, val loss 1.8393\nstep 6800: train loss 1.3321, val loss 1.8248\nstep 6900: train loss 1.3251, val loss 1.8137\nstep 7000: train loss 1.3207, val loss 1.8105\nstep 7100: train loss 1.3095, val loss 1.7979\nstep 7200: train loss 1.3389, val loss 1.8208\nstep 7300: train loss 1.3211, val loss 1.7852\nstep 7400: train loss 1.3193, val loss 1.8113\nstep 7500: train loss 1.3185, val loss 1.8143\nstep 7600: train loss 1.3173, val loss 1.8183\nstep 7700: train loss 1.3191, val loss 1.8131\nstep 7800: train loss 1.3206, val loss 1.7923\nstep 7900: train loss 1.3022, val loss 1.7816\nstep 8000: train loss 1.3022, val loss 1.7949\nstep 8100: train loss 1.2953, val loss 1.8006\nstep 8200: train loss 1.2989, val loss 1.8067\nstep 8300: train loss 1.2898, val loss 1.8000\nstep 8400: train loss 1.2928, val loss 1.8070\nstep 8500: train loss 1.2860, val loss 1.8001\nstep 8600: train loss 1.2986, val loss 1.8043\nstep 8700: train loss 1.2869, val loss 1.8100\nstep 8800: train loss 1.2833, val loss 1.7854\nstep 8900: train loss 1.2935, val loss 1.8020\nstep 9000: train loss 1.2798, val loss 1.8097\nstep 9100: train loss 1.2862, val loss 1.7941\nstep 9200: train loss 1.2801, val loss 1.7839\nstep 9300: train loss 1.2780, val loss 1.8103\nstep 9400: train loss 1.2906, val loss 1.8046\nstep 9500: train loss 1.2822, val loss 1.8033\nstep 9600: train loss 1.2734, val loss 1.7810\nstep 9700: train loss 1.2714, val loss 1.8044\nstep 9800: train loss 1.2748, val loss 1.7960\nstep 9900: train loss 1.2643, val loss 1.8026\nstep 9999: train loss 1.2674, val loss 1.7782\n    \\; \\bigcup_{i'\\in U'}_{\\lambda} (v_y\\times W_{x',0,k}).\n            \\item\n    Assume that $w=\\Theta)\n\\rightarrow (a,v)=\\{0,1\\}$. \n    \n To bumpple, that  this properts closed to  a parameter a correspond $Z$\\bm{W}$, where  there $\\Phi_x$.\n\\qed\n\\end{$(I-s_a,o}, D_j$.\n\n\\sum_{A}\n\n  \\ssc^\\infty\\in W\\setminus \\oplus W\\Q^+=\\wh{e}^2&@&lt;\\arrow{\\Gamma^\\ast}|_{x_{a_1\\wh{\\neq_x^*}_2 (a,v,\\tau(g'),l(o_i)\\to \\abs{\\tau (0)} \\leq \\a@ \\mathscr{C}^{-1}^{\\iota}\\phi^{\\tau_0}} \\times{WZ+HWZ8}(\\sigma)\\rightarrow \\mu (\\tau)\\circ T (-\\bf deffer to $t_X\\rightarrow {\\mathscr{S}$}_t {t_x}(y_{x_p})$.   If those proved that $E\\leq m$ local brive isomorphism, such. $X\\subset W$ with $f&gt;P_a(a,o_a)$\nwhere are istension.  Apparacompact maps  reall $\\zi$ and $\\lambda)'\\to (\\Gamma_a)\\circ\\frac{1}(F, \\alpha^+,\\alpha^+,-)$, defined to the image filted in $C_x$ has a tangent of $I$\\xi$ holows fromov-wing-fiber-compactness prove that $k=\\sigma$.\n\\qed\n\\end{definition}\n\nHere tangent $f'_k$ conclude type,  the map $(U, \\phi^{-1}): \\pi\n\\cU| \\mathscr{C}^{-1}(\\cF)\\to O^\\pm\\in \\cP(\\cW^{}_a; \\rightarrow \\abs{ b}_X :\\Tti-(\\rT_x \\bigl|)  \\cdot, t_H^* \\cap (p_-, A_+)$ and we such use that with are isots that solutions hysuality for the sensions of $\\tau$ of a zero defined only\nof next the point solution provese $\\abs{\\beta(a)}$ also linear operators,\nwhich, ands and the map \\rho$ are $ is b.\nHance we have this sc-smoothly of $X'_{{0,1}}\\oplus W$ of ep-groupoidal $z\\in \\w-hat{s}_{\\iota}'(\\Sigma)$) is sc-smooth, which nece orbifold with the equest closed on $\\delbar_x=\\ast\\ov\\circ s^{t,y$.\n\n\\item[(\\beta) \\begin{le}\\ \\\\ over(y)| \\alpha \\in F^\\ast_{x,y} +h}^+\\ast$ to equ\nsee. the morphism\n$\nw_{\\ast\\colon X} =(\\tau (\\bigl( \\phi & \\wh{V}_\\ast) (\\big )\\circ d_{m+i}(TD,x',\\tau,g)\n    (\\phi, \\psi \\exp_p \\bm{M}(\\phi, \\beta, {\\phi})\\rightarrow [0,1] \\to \\sum_{X_x'$-acts of a stable.   \n  In bhoose a finite-section be cholowse which  associated for whoosen \\ref{rm-stratned(3)-suborbifold}}} we have metrizable\n many, and the indices recalized t"
  },
  {
    "objectID": "fastai-1.html",
    "href": "fastai-1.html",
    "title": "Wolfgang Schmaltz",
    "section": "",
    "text": "::: {#cell-0 .cell _kg_hide-input=‘false’ _kg_hide-output=‘false’ execution=‘{“iopub.execute_input”:“2023-03-09T12:56:53.917785Z”,“iopub.status.busy”:“2023-03-09T12:56:53.916489Z”,“iopub.status.idle”:“2023-03-09T12:57:07.982541Z”,“shell.execute_reply”:“2023-03-09T12:57:07.981391Z”,“shell.execute_reply.started”:“2023-03-09T12:56:53.917689Z”}’ execution_count=1}\n:::\nIn this short notebook, I build a classifier to determine whether an image is a beetle or The Beatles!"
  },
  {
    "objectID": "fastai-1.html#step-1-download-images-of-beetles-and-the-beatles",
    "href": "fastai-1.html#step-1-download-images-of-beetles-and-the-beatles",
    "title": "Wolfgang Schmaltz",
    "section": "Step 1: Download images of beetles and The Beatles",
    "text": "Step 1: Download images of beetles and The Beatles\nUsing the python package ddg_search we begin by searching for images of beetles and The Beatles.\n\nfrom duckduckgo_search import ddg_images\nfrom fastcore.all import *\n\ndef search_images(term, max_images=30):\n    print(f\"Searching for '{term}'\")\n    return L(ddg_images(term, max_results=max_images)).itemgot('image')\n\n\nfrom fastdownload import download_url\nfrom fastai.vision.all import *\n\ndownload_url(search_images('beetles', max_images=1)[0], 'beetle.jpg', show_progress=False)\n#Image.open('beetle.jpg').to_thumb(256,256)\n\nSearching for 'beetles'\n\n\nPath('beetle.jpg')\n\n\n\ndownload_url(search_images('the Beatles', max_images=1)[0], 'the_beatles.jpg', show_progress=False)\n#Image.open('the_beatles.jpg').to_thumb(256,256)\n\nSearching for 'the Beatles'\n\n\nPath('the_beatles.jpg')\n\n\nWe now save searches for beetles and The Beetles into folders.\n\nsearches = 'beetles','the Beatles'\npath = Path('beetle_or_the_Beatles')\nfrom time import sleep\n\nfor o in searches:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True, parents=True)\n    download_images(dest, urls=search_images(f'{o}'))\n    sleep(10)  # Pause between searches to avoid over-loading server\n    resize_images(path/o, max_size=400, dest=path/o)\n\nSearching for 'beetles'\nSearching for 'the Beatles'"
  },
  {
    "objectID": "fastai-1.html#step-2-train-a-model",
    "href": "fastai-1.html#step-2-train-a-model",
    "title": "Wolfgang Schmaltz",
    "section": "Step 2: Train a model",
    "text": "Step 2: Train a model\nWe remove photos which didn’t download correctly.\n\nfailed = verify_images(get_image_files(path))\nfailed.map(Path.unlink)\nlen(failed)\n\n2\n\n\nWe use a ‘Datablock’ to separate the data into training and validation sets.\n\ndls = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=[Resize(192, method='squish')]\n).dataloaders(path, bs=32)\n\ndls.show_batch(max_n=6)\n\n\n\n\n\n\n\n\nWe now use ‘resnet18’ to train our model, and the ‘fine_tune()’ method from ‘fastai’ to tune the model.\n\nlearn = vision_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(3)\n\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n1.623580\n4.855602\n0.545455\n00:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\nerror_rate\ntime\n\n\n\n\n0\n0.892229\n3.018108\n0.545455\n00:04\n\n\n1\n0.693663\n0.710887\n0.272727\n00:04\n\n\n2\n0.487951\n0.266973\n0.090909\n00:04"
  },
  {
    "objectID": "fastai-1.html#step-3-test-the-model",
    "href": "fastai-1.html#step-3-test-the-model",
    "title": "Wolfgang Schmaltz",
    "section": "Step 3: Test the model",
    "text": "Step 3: Test the model\nWe can now test our model on the images of a beetle and of the Beatles we downloaded in Step 1.\n\nImage.open('beetle.jpg').to_thumb(256,256)\n\n\n\n\n\n\n\n\n\nis_beetle,_,probs = learn.predict('beetle.jpg')\nprint(f\"This is a: {is_beetle}.\")\nprint(f\"Probability it's a beetle: {probs[0]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: beetles.\nProbability it's a beetle: 1.0000\n\n\n\nImage.open('the_beatles.jpg').to_thumb(256,256)\n\n\n\n\n\n\n\n\n\nis_the_beatles,_,probs = learn.predict('the_beatles.jpg')\nprint(f\"This is a: {is_the_beatles}.\")\nprint(f\"Probability it's the Beatles: {probs[1]:.4f}\")\n\n\n\n\n\n\n\n\nThis is a: the Beatles.\nProbability it's the Beatles: 0.9777"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wolfgang Schmaltz",
    "section": "",
    "text": "I wanted to understand the world. So in school I took every science class available to me: your standard biology and chemistry courses, and also classical mechanics, astronomy and astrophysics, and then later, electromagnetism, special and general relativity, and quantum mechanics.\nI came to realize that if you want to understand anything, underneath it all, all there is, is mathematics. So I became a mathematician, and studied and understood things no one else had ever understood before. With time and effort I gained the intuition to think rigorously about “polyfolds”—abstract infinite-dimensional spaces allowing for degenerative “bubbling” phenomena. Using the methods and techniques of polyfold theory, for nearly a decade, I worked on a complete and rigorous proof of the Gromov–Witten axioms. My research was ultimately successful in solving this problem completely and in full generality. Thanks in part to my own work, there are very few interesting and tractable problems that remain in my sphere of mathematical expertise.\nMy interest has turned towards the technologies of our modern age. To me, the best way to keep abreast of new technological developments which have the power and promise to change the world is to fully understand them myself. Although I gladly leave the world of mathematics behind, the fact remains—mathematics is underneath everything!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Wolfgang Schmaltz",
    "section": "",
    "text": "I wanted to understand the world. So in school I took every science class available to me: your standard biology and chemistry courses, and also classical mechanics, astronomy and astrophysics, and then later, electromagnetism, special and general relativity, and quantum mechanics.\nI came to realize that if you want to understand anything, underneath it all, all there is, is mathematics. So I became a mathematician, and studied and understood things no one else had ever understood before. With time and effort I gained the intuition to think rigorously about “polyfolds”—abstract infinite-dimensional spaces allowing for degenerative “bubbling” phenomena. Using the methods and techniques of polyfold theory, for nearly a decade, I worked on a complete and rigorous proof of the Gromov–Witten axioms. My research was ultimately successful in solving this problem completely and in full generality. Thanks in part to my own work, there are very few interesting and tractable problems that remain in my sphere of mathematical expertise.\nMy interest has turned towards the technologies of our modern age. To me, the best way to keep abreast of new technological developments which have the power and promise to change the world is to fully understand them myself. Although I gladly leave the world of mathematics behind, the fact remains—mathematics is underneath everything!"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Wolfgang Schmaltz",
    "section": "Education",
    "text": "Education\nPhD in Mathematics\nUniversity of California, Berkeley; Berkeley, CA\n16 August 2012 - 11 May 2018\nB.S. in Mathematics\nUniversity of Chicago; Chicago, IL\n24 September 2007 - 11 June 2011"
  },
  {
    "objectID": "index.html#postdoctoral-appointments",
    "href": "index.html#postdoctoral-appointments",
    "title": "Wolfgang Schmaltz",
    "section": "Postdoctoral Appointments",
    "text": "Postdoctoral Appointments\nFaculty of Mathematics\nRuhr-Universität Bochum; Bochum, Germany\n1 March 2020 - 31 December 2024\nMathematics Institute\nJustus-Liebig University; Gießen, Germany\n1 January 2018 - 29 February 2020"
  },
  {
    "objectID": "index.html#papers",
    "href": "index.html#papers",
    "title": "Wolfgang Schmaltz",
    "section": "Papers",
    "text": "Papers\nPseudocycle Gromov–Witten invariants are a strict subset of polyfold Gromov–Witten invariants\nNon-fillability of overtwisted contact manifolds via polyfolds (joint with S. Suhr, K. Zehmisch)\nThe Gromov–Witten axioms for symplectic manifolds via polyfold theory\nNaturality of polyfold invariants and pulling back abstract perturbations\nThe Steenrod problem for orbifolds and polyfold invariants as intersection numbers"
  },
  {
    "objectID": "index.html#invited-talks",
    "href": "index.html#invited-talks",
    "title": "Wolfgang Schmaltz",
    "section": "Invited Talks",
    "text": "Invited Talks\nOrbifold Intersection Theory and Polyfold Gromov–Witten Invariants\n28 November 2023; Heidelberg Geometry Seminar; Heidelberg, Germany\nPseudocycle Gromov–Witten Invariants \\(\\subsetneq\\) Polyfold Gromov–Witten Invariants\n11 February 2021; Arbeitsgemeinschaft; Bochum, Germany\nThe Steenrod problem for orbifolds and polyfold Gromov–Witten invariants\n10 December 2020; Oberseminar Dynamische Systeme; Bochum, Germany\nGromov–Witten Axioms for Symplectic Manifolds via Polyfold Theory\n15 April 2019; Berlin-Hamburg-Seminar; Berlin, Germany\nGromov–Witten Axioms for Symplectic Manifolds via Polyfold Theory\n24 October 2018; SISSA; Trieste, Italy\nGromov–Witten Axioms for Symplectic Manifolds via Polyfold Theory\n30 August 2018; Symplectic Field Theory IX: Polyfolds for SFT; Augsburg, Germany\nGromov–Witten Axioms for Symplectic Manifolds via Polyfold Theory\n5 July 2018; Justus-Liebig University; Gießen, Germany\nNaturality of Polyfold Invariants\n2 May 2018; Mathematical Sciences Research Institute; Berkeley, California\nThe Polyfold Gromov–Witten Invariants & Gromov–Witten Axioms for Symplectic Manifolds via Polyfold Theory\n25 April 2018; Berkeley Topology Seminar; Berkeley, California"
  },
  {
    "objectID": "deep_learning.html",
    "href": "deep_learning.html",
    "title": "DEEP LEARNING!",
    "section": "",
    "text": "Less is more.\nImplement and demonstrate a ‘which beetle are you’ classifier\nDecision Tree with Visualization\n\n\n\n\nBuild a model on some data\n\n\n\n\n\nFramework from Rob Mulla\nFacebook Prophet\nBest results from kaggle course\nPredict energy market time series data via timeseriesAI\n\nComment on Max Dama’s quote on machine learning\n\n\n\n\n\n\nBuild a miniGPT?\n\n\n\n\n\nPlay with Stable Diffusion"
  },
  {
    "objectID": "deep_learning.html#hand-made-neural-network",
    "href": "deep_learning.html#hand-made-neural-network",
    "title": "DEEP LEARNING!",
    "section": "",
    "text": "Build a model on some data"
  },
  {
    "objectID": "deep_learning.html#time-series-analysis",
    "href": "deep_learning.html#time-series-analysis",
    "title": "DEEP LEARNING!",
    "section": "",
    "text": "Framework from Rob Mulla\nFacebook Prophet\nBest results from kaggle course\nPredict energy market time series data via timeseriesAI\n\nComment on Max Dama’s quote on machine learning"
  },
  {
    "objectID": "deep_learning.html#natural-language-processing",
    "href": "deep_learning.html#natural-language-processing",
    "title": "DEEP LEARNING!",
    "section": "",
    "text": "Build a miniGPT?"
  },
  {
    "objectID": "deep_learning.html#stable-diffusion",
    "href": "deep_learning.html#stable-diffusion",
    "title": "DEEP LEARNING!",
    "section": "",
    "text": "Play with Stable Diffusion"
  },
  {
    "objectID": "climbing.html#the-5-classic-7as",
    "href": "climbing.html#the-5-classic-7as",
    "title": "Climbing",
    "section": "The 5 Classic 7As",
    "text": "The 5 Classic 7As"
  },
  {
    "objectID": "climbing.html#the-long-legs-of-gullich",
    "href": "climbing.html#the-long-legs-of-gullich",
    "title": "Climbing",
    "section": "The Long Legs of Gullich",
    "text": "The Long Legs of Gullich"
  },
  {
    "objectID": "climbing.html#italy---ragno-di-mare",
    "href": "climbing.html#italy---ragno-di-mare",
    "title": "Climbing",
    "section": "Italy - Ragno di Mare",
    "text": "Italy - Ragno di Mare"
  },
  {
    "objectID": "climbing.html#the-wilder-kaiser",
    "href": "climbing.html#the-wilder-kaiser",
    "title": "Climbing",
    "section": "The Wilder Kaiser",
    "text": "The Wilder Kaiser"
  },
  {
    "objectID": "hello.html",
    "href": "hello.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "hello.html#polar-axis",
    "href": "hello.html#polar-axis",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "fastai-3.html",
    "href": "fastai-3.html",
    "title": "Comparing computer vision models",
    "section": "",
    "text": "As newcomers in the world of deep learning, we are told that we can generally treat pre-trained computer vision models as ‘black boxes’, without understanding the inner workings of the models. On this page, we will compare the performance of some of the state-of-the-art computer vision models. In doing so, we gain a better mental map of what performance looks like on the cutting edge, as well as demonstrating some of the visualization tools of data analysis.\nWe use the state-of-the-art computer vision models from PyTorch Image Models (timm). Our benchmarks for these models were collected by Ross Wightman and come from his GitHub. Our analysis is based on Jeremey Howard’s orignal analysis."
  },
  {
    "objectID": "fastai-3.html#commentary-on-findings",
    "href": "fastai-3.html#commentary-on-findings",
    "title": "Comparing computer vision models",
    "section": "Commentary on findings",
    "text": "Commentary on findings\nThe LeViT family models are both fast and accurate. Apparently these models are constructed using a hybrid of convolution neural networks and transformets.\nThe Swin family of transformers is apparently among the most accurate. It is described as a “hierarchical Transformer whose representation is computed with shifted windows.”"
  },
  {
    "objectID": "fastai-3.html#speed-vs-parameter-count",
    "href": "fastai-3.html#speed-vs-parameter-count",
    "title": "Comparing computer vision models",
    "section": "Speed vs parameter count",
    "text": "Speed vs parameter count\nWe finally compare speed vs parameter count. Often, parameter count is used in papers as a proxy for speed. However, as we see, there is a wide variation in speeds at each level of parameter count, so it’s really not a useful proxy. There is sometimes a correlation between parameter count and needed memory, but this is also not always so useful. In the following chart: - the x axis shows the parameter count in a log scale - the y axis shows the speed in seconds in a log scale.\n\npx.scatter(df, width=w, height=h,\n    x='param_count_x',  y='secs', log_x=True, log_y=True, color='infer_img_size',\n    hover_name='model', hover_data=['infer_samples_per_sec', 'family']\n)"
  },
  {
    "objectID": "limitations.html",
    "href": "limitations.html",
    "title": "Mathematics and the Limitations of Modern Artifical Intelligence",
    "section": "",
    "text": "Some years ago, at one of the UC Berkeley afternoon teas I was speaking with a colleague of mine. He had just returned from a conference on “New Developments in Microlocal Sheaf Theory” (or something along those lines, I do not remember this precisely). What I do remember, vividly, is his description of a fascinating encounter. At the conference was a child, only 12-year-old, a rising prodigy in algebraic geometry, surrounded by people at least double his age. The colleague was naturally intrigued, and did his best to converse with the kid.\n“It was the strangest thing. He seemed to know the entirety of Hartshorne’s Algebraic Geometry by heart. He could rattle of the exact statement of, for example, the Lemma of Hironaka if you asked him.”\n“But there was no understanding of the meaning behind the words. I have no doubt he was a bright kid, with potential, so no offense by any means. But memorizing a theorem is not the same as understanding it.”\nThe above story presents a dichotomy, that of knowing vs. understanding. When we evaluate the capabilities and potential of modern Artificial Intelligence amidst the endless hype, we really ought to keep this fundamental contrast in mind."
  },
  {
    "objectID": "limitations.html#the-conventional-wisdom-ai-mathematicians-are-imminent",
    "href": "limitations.html#the-conventional-wisdom-ai-mathematicians-are-imminent",
    "title": "Mathematics and the Limitations of Modern Artifical Intelligence",
    "section": "The conventional wisdom: AI mathematicians are imminent",
    "text": "The conventional wisdom: AI mathematicians are imminent\n\nIs automation and economic disruption, long a concern for blue collar factory workers, coming for the jobs of academic mathematicians? According to the conventional wisdom, Artificial Intelligence will someday soon replace humans for all information-related tasks. Pointedly, this supposedly includes mathematics, ranging from simple calculations to Putnam and IMO problems to research level mathematics.\nThe New York Times informs us that A.I. Is Coming for Mathematics, Too, and suggests that the deductive jumps of reasoning required by mathematics are already being replicated by the impenetrable black boxes of neural networks. In an anecdote, a neural network is described as managing to correctly predict some desired mathematical constant, but unable to further elaborate or explain its conclusion: “the neural net had somehow intuitively discerned a mathematical truth, but the logical ‘why’ of it was far from obvious”. Inspection of the layers of the neural network proved predictably fruitless.\nThe Fields medalist Terrence Tao describes his own experience toying with GPT-4 as being “roughly on par with trying to advise a mediocre, but not completely incompetent, (static simulation of a) graduate student”; given a complex analysis problem, and with hints and prodding, it was able to give an accurate proof.\nEntering the realm of science-fiction, the prominent computer scientist and artificial intelligence researcher Christian Szegedy optimistically predicts that by June 2026 we will see “super-human mathematician AI”.\n\nHowever.\nDespite the breathless hype."
  },
  {
    "objectID": "limitations.html#the-current-reality-ai-cannot-reliably-solve-math-problems",
    "href": "limitations.html#the-current-reality-ai-cannot-reliably-solve-math-problems",
    "title": "Mathematics and the Limitations of Modern Artifical Intelligence",
    "section": "The (current) reality: AI cannot reliably solve math problems",
    "text": "The (current) reality: AI cannot reliably solve math problems\n\nPlaying around with any of the top of the line chatbots, it is easy to be initially impressed. They can convincingly (and correctly) compute a complicated integral requiring complex analysis, or even write a proof of Weinstein’s neighborhood theorem!\nPlaying around a little more, it isn’t hard at all for chatbots to produce gibberish; responses usually contain horrific basic arithmetical errors, hallucinations of connections between unrelated concepts, or unconditional overconfidence in the veracity of the given answer. (See the next sections for some illustrative experiments!)\nSo, are chatbots right now deducing the answers using logic and reasoning, arriving at the answer deductive step by deductive step? Or are they regurgitating tweaked answers they have seen before, after ingesting the world’s online libraries? OpenAI’s research LLM, o1-preview, showed a 30% drop in accuracy on a benchmark based on Putnam competition problems simply by tweaking the given questions.\nSerious skepticism about model performance on benchmarks is also warranted when companies appear to have few scruples regarding secretly gaming the results. For example, on December 20, 2024, OpenAI reported that its new research model o3 scored a 25.2% score on the T1 tier of Epoch AI’s FrontierMath benchmark 1 2. Unbeknownst to the many of the mathematicians who contributed to the benchmark, OpenAI has funded Epoch AI, and was given exclusive access to the problems and solutions of 250 problems, with a hold out set of 50 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo illustrate my points, and just for fun, let’s look at how some of the top chatbots perform when asked some math problems."
  },
  {
    "objectID": "mathematics.html",
    "href": "mathematics.html",
    "title": "Mathematics",
    "section": "",
    "text": "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Standard Commands %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%mathbb \n%greek \n%GREEK \n%cal \n%tilde \n%bar % \n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % Math Commands %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n%compactified moduli space \n% % % % %SYMPLECTIC GEOMETRY % % % % % \n% % % % %Hat Delbar % % % % % \n% % % % %COLORS % % % % %\n% % % % %GROMOV-WITTEN LANGUAGE % % % % % \n% % % % %THOM LANGUAGE % % % % % \n% % % % %POLYFOLD LANGUAGE % % % % %"
  },
  {
    "objectID": "mathematics.html#my-research-accomplishments",
    "href": "mathematics.html#my-research-accomplishments",
    "title": "Mathematics",
    "section": "My research accomplishments",
    "text": "My research accomplishments\nIn 1994, Kontsevich and Manin stated the Gromov–Witten axioms. At the time, it was not possible for them to give a complete proof; the field of Gromov–Witten theory was not yet mature, and it would take years for a mathematically rigorous definition of a Gromov–Witten invariant to develop. In 2017, polyfold theory, as developed by Hofer, Wysocki, and Zehnder, was successful in giving a well-defined Gromov–Witten invariant for general symplectic manifolds.\nThe focus of my mathematical career has been in giving a complete proof of the Gromov–Witten axioms using the tools and techniques of polyfold Gromov–Witten invariants. In 2019, I succeeded in this research project.\nMy research accomplishments to date are the following:\n\nFollowing the work of Thom, I solved the Steenrod problem for closed orientable orbifolds, proving that the rational homology groups of a closed orientable orbifold have a basis consisting of classes represented by closed embedded full suborbifolds whose normal bundles have fiberwise trivial isotropy action.\nI proved that the polyfold Gromov–Witten invariants, originally defined via integration of differential forms, may equivalently be defined as intersection numbers against a basis of representing suborbifolds.\nI provided a general framework for proving that polyfold invariants are natural, and in particular demonstrated that the polyfold Gromov–Witten invariants do not depend on auxiliary choices.\nI constructed the universal curve polyfold on which it is possible to define the \\(k\\)th-marked point forgetting map and to pullback abstract perturbations.\nI gave a complete proof of the Gromov–Witten axioms for the polyfold Gromov–Witten invariants.\nI showed that the classical pseudocycle Gromov–Witten invariants, defined for semipositive symplectic manifolds, are a strict subset of the more general polyfold Gromov–Witten invariants."
  },
  {
    "objectID": "mathematics.html#history-of-the-gromovwitten-axioms",
    "href": "mathematics.html#history-of-the-gromovwitten-axioms",
    "title": "Mathematics",
    "section": "History of the Gromov–Witten axioms",
    "text": "History of the Gromov–Witten axioms\nIn 1985 Gromov published the paper “Pseudo holomorphic curves in symplectic manifolds”, laying the foundations for the modern study of pseudo holomorphic curves (also know as \\(J\\)-holomorphic curves) in symplectic topology (Gromov 1985). In this paper, Gromov proved a compactness result for the moduli space of \\(J\\)-holomorphic curves in a fixed homology class. This paper contained antecedents to the modern notion of the Gromov–Witten invariants in the proofs of the nonsqueezing theorem and the uniqueness of symplectic structures on \\(\\mathbb{C}P^2\\).\nAround 1988, inspired by Floer’s study of gauge theory on three manifolds, Witten introduced the topological sigma model (Floer 1988; Witten 1988). The invariants of this model are the “\\(k\\)-point correlation functions”, another precursor to the modern notion of the Gromov–Witten invariants. Witten also observed some of the relationships between these invariants and possible degenerations of Riemann surfaces (Witten 1991). Further precursors to the notion of the Gromov–Witten invariants can also be seen in McDuff’s classification of symplectic ruled surfaces (McDuff 1991).\nIn 1993 Ruan gave a modern definition of the genus-zero Gromov–Witten invariants for semipositive symplectic manifolds (Ruan 1996, 1994). At the end of 1993, Ruan and Tian established the associativity of the quantum product for semipositive symplectic manifolds, giving a mathematical basis to the composition law of Witten’s topological sigma model (Ruan and Tian 1995).\nIn 1994 Kontsevich and Manin stated the Gromov–Witten axioms, given as a list of formal relations between the Gromov–Witten invariants (Kontsevich and Manin 1994). At the time it was not possible for Kontsevich and Manin to give a proof of the relations they listed; the definition of the Gromov–Witten invariant (complete with homology classes from a Deligne–Mumford space) would require in addition new ideas involving “stable maps” (Kontsevich 1995). Hence they used to term “axiom” with the presumed meaning “to take for assumption without proof”/ “to use as a premise for further reasoning”. And indeed, from these starting assumptions they were able to establish foundational results in enumerative geometry, answers to esoteric questions such as:\n\nKontsevich’s recursion formula. Let \\(d\\geq 1\\). How many degree \\(d\\) rational curves in \\(\\mathbb{C}P^2\\) pass through \\(3d - 1\\) points in general position?\n\nMoreover, in this paper they outlined some of the formal consequences of the axioms by demonstrating how to combine the invariants into a Gromov–Witten potential, and interpret the axioms as differential equations which the potential satisfies.\nTo varying extents, this work has predated the construction of a well-defined Gromov–Witten invariant in symplectic geometry for \\(J\\)-holomorphic curves of arbitrary genus, and for all closed symplectic manifolds. Efforts to construct a well-defined Gromov–Witten invariant constitute an ever growing list of publications, including but not limited to the following: (Li and Tian 1998; Fukaya and Ono 1999; Fukaya et al. 2012; Siebert 1996; Cieliebak and Mohnke 2007; McDuff and Wehrheim 2012, 2018, 2017; Ionel and Parker 2013; Pardon 2016). A discussion of some of the difficulties inherent in these approaches can be found in (Fabert et al. 2016). Similarly, there have been several efforts to prove the Gromov–Witten axioms (Fukaya and Ono 1999; McDuff and Salamon 2012; Castellano 2016).\nOver the past two decades, Hofer, Wysocki, and Zehnder have developed a new approach to resolving transversality issues that arise in the study of \\(J\\)-holomorphic curves in symplectic geometry called polyfold theory (Hofer, Wysocki, and Zehnder 2007, 2009a, 2009b, 2017a, 2010b, n.d., 2010a, 2017b). This approach has been successful in constructing a well-defined Gromov–Witten invariant for arbitrary genus and for general symplectic manifolds (Hofer, Wysocki, and Zehnder 2017a).\nWith a fully general Gromov–Witten invariant finally defined rigorously, my research has fixated on answering the following question:\n\nIs it finally possible to prove the Gromov–Witten axioms by using polyfold theory?"
  },
  {
    "objectID": "mathematics.html#what-is-a-gromovwitten-invariant",
    "href": "mathematics.html#what-is-a-gromovwitten-invariant",
    "title": "Mathematics",
    "section": "What is a Gromov–Witten invariant?",
    "text": "What is a Gromov–Witten invariant?\nLet \\((M,\\omega)\\) be a closed symplectic manifold of dimension \\(\\dim M = 2n\\), and let \\(J\\) be a \\(\\omega\\)-compatible almost complex structure. For a fixed homology class \\(A\\in H_2(M,{\\mathbb Z})\\), and for fixed integers \\(g\\geq 0\\), \\(k\\geq 0\\), we consider the set of \\(J\\)-holomorphic curves:\n\\[\n    {\\mathcal M}_{A,g,k}(J) :=\n    \\left\\{\n    \\begin{array}{c}\n        u: (\\Sigma_g,j) \\to M \\\\\n        \\{z_1,\\ldots,z_k\\}\\in \\Sigma_g\n    \\end{array}\n    \\biggm|\n    \\begin{array}{c}\n        \\tfrac{1}{2} (du+J\\circ du\\circ j)=0 \\\\\n        u_*[\\Sigma_g] = A\n    \\end{array}\n    \\right\\}\n    \\biggm/\n    \\begin{array}{l}\n        u \\sim u\\circ \\phi, \\\\\n        \\phi\\in \\text{Aut}\n    \\end{array}\n\\]\nconsisting of smooth maps \\(u:(\\Sigma_g,j)\\to M\\) which satisfy the Cauchy–Riemann equation modulo reparametrization; here \\((\\Sigma_g,j)\\) is a genus \\(g\\) Riemann surface and \\(\\text{Aut}\\) is the automorphism group of the Riemann surface \\((\\Sigma_g,j)\\) which preserves the ordering of the marked points.\nGromov proved this set has a natural compactification in (Gromov 1985), which was later refined into the stable map compactification of Kontsevich in (Kontsevich 1995), and thus we may also consider its compactification, the Gromov–Witten moduli space:\n\\[\n    \\bar{\\mathcal{M}}_{A,g,k} (J) := {\\mathcal M}_{A,g,k} (J) \\sqcup \\{\\text{stable nodal $J$-holomorphic curves}\\}.\n\\]\nWe seek to use this space to construct invariants of the symplectic manifold \\(M\\). To this end, we define the evaluation map which evaluates a stable curve on each marked point:\n\\[\n    ev: \\bar{\\mathcal{M}}_{A,g,k} (J) \\to M\\times \\cdots \\times M.\n\\]\nOn the top stratum of non-noded stable \\(J\\)-holomorphic curves it is given by\n\\[\n    ev\\left([(u,z_1,\\ldots,z_k)] \\right): = (u(z_1),\\ldots, u(z_k)).\n\\]\nWith the fixed integers \\(g \\geq 0\\), \\(k \\geq 3\\) we also consider the associated Deligne–Mumford space, the natural compactification of the space of configurations of a complex structure and \\(k\\)-marked points on a genus \\(g\\) Riemann surface modulo biholomorphic equivalence:\n\\[\n    \\bar{\\mathcal{M}}_{g,k} := \\text{cl} \\left(\\{ j, \\{z_1,\\ldots,z_k\\}\\in \\Sigma_g \\mid j \\text{ complex structure on } \\Sigma_g, z_i\\neq z_j \\text{ if } i \\neq j\\} / \\text{Aut} \\right).\n\\]\nWhen \\(g = 0\\) this space is a finite-dimensional manifold, and when \\(g\\neq 0\\) this space is a finite-dimensional orbifold, in either case of dimension \\(\\text{dim}\\) \\(\\bar{\\mathcal{M}}_{g,k} = 6g - 6 + 2k\\) We may define a projection map from the Gromov–Witten moduli space to the Deligne–Mumford space which forgets the curve which maps to \\(M\\) and which stabilizes the resulting unstable domain components:\n\\[\n    \\pi: \\bar{\\mathcal{M}}_{A,g,k} (J) \\to \\bar{\\mathcal{M}}_{g,k}.\n\\]\nOn the top stratum of non-noded stable \\(J\\)-holomorphic curves it forgets the map \\(u\\) and is given by\n\\[\n    \\pi\\left([(u,j,z_1,\\ldots,z_k)]\\right) := [(j,z_1,\\ldots,z_k)].\n\\]\nThe traditional interpretation of a Gromov–Witten invariant is the (supposedly) finite count of \\(J\\)-holomorphic curves which at the \\(i\\)th-marked point pass through a submanifold \\({\\mathcal X}_i \\subset M\\) and whose marked point configuration is restricted by the projection map to a suborbifold \\({\\mathcal B}\\subset \\bar{\\mathcal{M}}_{g,k}\\). This can be visualized as the intersection number of the Gromov–Witten moduli space \\(\\bar{\\mathcal{M}}_{A,g,k}(J)\\) with the product suborbifold \\({\\mathcal X}_1\\times \\cdots \\times {\\mathcal X}_k \\times {\\mathcal B}\\) via the map \\(ev_1\\times \\cdots \\times ev_k \\times \\pi\\) as depicted in the following diagram:\n\\[\n\\begin{align*}\n\\bar{\\mathcal{M}}_{A,g,k}(J) \\xrightarrow{ev_1\\times \\cdots \\times ev_k \\times \\pi} M\\times \\cdots & \\times M \\times \\bar{\\mathcal{M}}_{g,k} \\\\\n                                                                                                                                                    & \\cup \\\\\n                                                                                                                    {\\mathcal X}_1\\times \\cdots &\\times {\\mathcal X}_k \\times {\\mathcal B}.\n\\end{align*}\n\\]\nSuch an intersection number should depend only on the homology classes of the submanifolds / suborbifolds, and should be independent of the almost complex structure. This count can be packaged algebraically as a homomorphism:\n\\[\n    \\mathop{\\mathrm{GW}}_{A,g,k} : H_*(M;{\\mathbb Q})^{\\otimes k} \\times H_*(\\bar{\\mathcal{M}}_{g,k};{\\mathbb Q}) \\to {\\mathbb Q}.\n\\]\nA foundational problem in symplectic geometry is to actually show that such a Gromov–Witten invariant is well-defined. Ideally, we would like to define a Gromov–Witten invariant rigorously via an intersection number:\n\\[\n\\mathop{\\mathrm{GW}}_{A,g,k} ([{\\mathcal X}_1],\\ldots,[{\\mathcal X}_k];[{\\mathcal B}]) = (ev\\times \\pi) (\\bar{\\mathcal{M}}_{A,g,k} (J)) \\cdot ({\\mathcal X}_1\\times \\cdots \\times {\\mathcal X}_k \\times {\\mathcal B}),\n\\]\nor as an integral:\n\\[\n\\mathop{\\mathrm{GW}}_{A,g,k} ([{\\mathcal X}_1],\\ldots,[{\\mathcal X}_k];[{\\mathcal B}]) = \\int_{\\bar{\\mathcal{M}}_{A,g,k}(J)} ev^* (\\mathop{\\mathrm{PD}}[{\\mathcal X}_1]\\wedge \\cdots \\wedge \\mathop{\\mathrm{PD}}[{\\mathcal X}_k]) \\wedge \\pi^* \\mathop{\\mathrm{PD}}[{\\mathcal B}],\n\\]\nor as a pairing with a (virtual) fundamental class:\n\\[\n\\mathop{\\mathrm{GW}}_{A,g,k} ([{\\mathcal X}_1],\\ldots,[{\\mathcal X}_k];[{\\mathcal B}]) = \\left\\langle (ev\\times\\pi)_* [\\bar{\\mathcal{M}}_{A,g,k}(J)], \\mathop{\\mathrm{PD}}[{\\mathcal X}_1\\times\\cdots\\times{\\mathcal X}_k\\times{\\mathcal B}] \\right\\rangle.\n\\]\nSuch definitions require additional structure on the Gromov–Witten moduli space; an intersection number requires tangent spaces and notions of transversal intersection, an integral requires smooth partitions of unity and notions of differential forms, and a (virtual) fundamental class requires a distinguished homology class on the topological space.\nHowever, a priori, the Gromov–Witten moduli space only has the structure of a compact topological space, and this alone is insufficient to define any of the above. More structure is needed."
  },
  {
    "objectID": "mathematics.html#the-classical-pseudocycle-gromovwitten-invariant",
    "href": "mathematics.html#the-classical-pseudocycle-gromovwitten-invariant",
    "title": "Mathematics",
    "section": "The classical pseudocycle Gromov–Witten invariant",
    "text": "The classical pseudocycle Gromov–Witten invariant\nThe classical approach to defining the Gromov–Witten invariants is to show that the Gromov–Witten moduli space has the structure of a pseudocycle.\nIf the Cauchy–Riemann section were completely transversal to the zero section, i.e., transversal when considered on all possible nodal strata of the space of \\(J\\)-curves, then the top stratum of the space of \\(J\\)-curves would have the structure of a finite-dimensional manifold and all nodal strata would have the structure of manifolds with codimension at least \\(2\\) relative to the top stratum. Thus the space of \\(J\\)-curves would have the structure of a “pseudocycle”, i.e., a space whose boundary (the nodal strata) would be invisible from the point of view of homology.\nDefinition. Let \\(M\\) be a smooth manifold. A smooth map \\(f: V \\to M\\) is a \\(d\\)-dimensional pseudocycle if \\(V\\) is an oriented \\(d\\)-dimensional manifold \\(V\\) such that the image \\(f(V)\\) has compact closure and such that the image of the boundary \\(f(\\bar{V} \\setminus V)\\) has dimension at most \\(\\leq d - 2\\).\nPseudocycles have sufficient structure for defining intersection numbers, as for dimension reasons the boundary will not contribute to the intersection number.\nHowever, transversality of the Cauchy–Riemann section is often impossible to obtain through classical techniques, i.e., perturbation of an almost complex structure. As a consequence, the nodal strata may have dimension larger than expected—indeed, larger than the dimension of the top stratum. This situation is a fundamental obstacle to the rigorous definition of a Gromov–Witten invariant; after all, for dimension reasons, any (nonzero) contribution to the intersection number from such a nodal stratum with large dimension would no longer be finite.\nThe pseudocycle approach to defining Gromov–Witten invariants deals with this difficulty by imposing strict conditions on the symplectic manifold which disallows such phenomena in the nodal strata. The “semipositive” condition was first introduced by McDuff in 1991 in (McDuff 1991): a symplectic manifold \\((M^{2n},\\omega)\\) is called semipositive if, for every \\(A\\in \\pi_2(M)\\),\n\\[\n    \\omega(A) &gt;0,\\ c_1(A) \\geq 3-n \\quad \\implies \\quad c_1(A) \\geq 0.\n\\]\nThis condition may seem ad-hoc, but it is specifically designed to guarantee that in the genus-zero case, the strata of nodal \\(J\\)-curves will have codimension at least \\(2\\) relative to the dimension of the top stratum of non-noded simple \\(J\\)-curves.\nTheorem. (McDuff and Salamon 2012, Thms. 6.6.1, 6.7.1) Let \\((M,\\omega)\\) be a semipositive sympletic manifold. There exists a regular almost complex structure on \\(M\\) such that the evaluation map from the genus-zero Gromov–Witten moduli space to the product \\(M\\times \\cdots \\times M,\\)\n\\[\n    ev: \\bar{\\mathcal{M}}_{A,0,k}(J) \\to M\\times\\cdots\\times M,\n\\]\nis a pseudocycle.\nThe pseudocycle Gromov–Witten invariant is the homomorphism\n\\[\n    \\text{pseudocycle-}\\mathop{\\mathrm{GW}}_{A,0,k} : H_* (M;{\\mathbb Q})^{\\otimes k} \\to {\\mathbb Q}\n\\]\ndefined via the intersection number of the pseudocycle \\(ev : {\\mathcal M}^*_{A,0,k} (J) \\to M\\times\\cdots\\times M\\) with a basis of representing submanifolds \\({\\mathcal X}\\subset M\\):\n\\[\n    \\text{pseudocycle-}\\mathop{\\mathrm{GW}}_{A,0,k} ([{\\mathcal X}_1],\\ldots,[{\\mathcal X}_k]) :=\n    ev({{\\mathcal M}^*_{A,0,k}(J)}) \\cdot \\left({\\mathcal X}_1 \\times\\cdots\\times {\\mathcal X}_k \\right).\n\\]\nThe invariant does not depend on the choice of regular almost complex structure \\(J\\), nor on the choice of representing basis."
  },
  {
    "objectID": "mathematics.html#the-polyfold-gromovwitten-invariant",
    "href": "mathematics.html#the-polyfold-gromovwitten-invariant",
    "title": "Mathematics",
    "section": "The polyfold Gromov–Witten invariant",
    "text": "The polyfold Gromov–Witten invariant\nPolyfold theory, developed by Hofer, Wysocki, and Zehnder, is a modern approach to resolving transversality issues that arise in attempts to solve moduli space problems in symplectic geometry. The polyfold theoretic approach to solving a moduli space problem is to recast the problem into familiar terms from differential geometry. To do this, we may construct a “Gromov–Witten polyfold” \\({\\mathcal Z}_{A,g,k}\\)—a massive, infinite-dimensional ambient space, designed to contain the entire unperturbed Gromov–Witten moduli space \\(\\bar{\\mathcal{M}}_{A,g,k}(J)\\) as a compact subset. We may furthermore construct a “strong polyfold bundle” \\({\\mathcal W}_{A,g,k}\\) over \\({\\mathcal Z}_{A,g,k}\\); the Cauchy–Riemann operator then defines a “scale smooth Fredholm section” of this bundle, \\(\\bar{\\partial}_J:{\\mathcal Z}_{A,g,k} \\to {\\mathcal W}_{A,g,k}\\), such that \\(\\smash{\\bar{\\partial}_J}\\vphantom{\\partial}^{-1}(0) = \\bar{\\mathcal{M}}_{A,g,k}(J)\\). We can construct “abstract perturbations” \\(p\\) of this section such that \\(\\bar{\\partial}_J+p\\) is transverse to the zero section and such that \\((\\bar{\\partial}_J+p)^{-1}(0)\\) is a compact set. In this way, we may take a scale smooth Fredholm section and “regularize” the unperturbed Gromov–Witten moduli space yielding a perturbed Gromov–Witten moduli space \\({\\mathcal S}_{A,g,k}(p):= (\\bar{\\partial}_J+p)^{-1}(0)\\) which has the structure of a compact oriented “weighted branched orbifold”.\n\\[\n        \\begin{array}{c}\n        \\bar{\\mathcal{M}}_{A,g,k}(J) = \\bar{\\partial}_J^{-1}(0)        \\\\\n        \\text{\\small{compact topological space}} \\\\\n        \\end{array}\n        \\xrightarrow{\\text{``polyfold regularization''}}\n        \\begin{array}{c}\n            {\\mathcal S}_{A,g,k}(p):=(\\bar{\\partial}_J+p)^{-1}(0) \\\\\n            \\text{\\small{compact ``weighted branched orbifold''}}\n        \\end{array}\n\\]\nThis approach has been successful in giving a well-defined Gromov–Witten invariant for curves of arbitrary genus, and for all closed symplectic manifolds.\nConsider homology classes \\(\\alpha_1,\\ldots, \\alpha_k \\in H_* (M;{\\mathbb Q})\\) and \\(\\beta\\in H_* (\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k};{\\mathbb Q})\\). We can represent the Poincar'e duals of the \\(\\alpha_i\\) and \\(\\beta\\) by closed differential forms in the de Rahm cohomology groups, \\(\\mathop{\\mathrm{PD}}(\\alpha_i)\\in H^*_{\\mathop{\\mathrm{dR}}} (M)\\) and \\(\\mathop{\\mathrm{PD}}(\\beta)\\in H^*_{\\mathop{\\mathrm{dR}}}(\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k})\\). By pulling back via the evaluation and projection maps, we obtain a closed \\(\\text{sc}\\)-smooth differential form\n\\[\n    ev_1^* \\mathop{\\mathrm{PD}}(\\alpha_1) \\wedge \\cdots \\wedge ev_k^* \\mathop{\\mathrm{PD}}(\\alpha_k) \\wedge\\pi^* \\mathop{\\mathrm{PD}}(\\beta) \\in H^*_{\\mathop{\\mathrm{dR}}} ({\\mathcal Z}_{A,g,k}).\n\\]\nTheorem. (Hofer, Wysocki, and Zehnder 2017a, Thm. 1.12) The polyfold Gromov–Witten invariant is the homomorphism\n\\[\n        \\mathop{\\mathrm{GW}}_{A,g,k} : H_* (M;{\\mathbb Q})^{\\otimes k} \\otimes H_* (\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}; {\\mathbb Q}) \\to {\\mathbb Q}\n\\]\ndefined via the “branched integration” of (Hofer, Wysocki, and Zehnder 2010a):\n\\[\n        \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1,\\ldots,\\alpha_k;\\beta) : = \\int_{{\\mathcal S}_{A,g,k}(p)} ev_1^* \\mathop{\\mathrm{PD}}(\\alpha_1) \\wedge \\cdots \\wedge ev_k^* \\mathop{\\mathrm{PD}}(\\alpha_k) \\wedge\\pi^* \\mathop{\\mathrm{PD}}(\\beta).\n\\]\nThis invariant does not depend on the choice of perturbation."
  },
  {
    "objectID": "mathematics.html#orbifolds-and-the-steenrod-problem",
    "href": "mathematics.html#orbifolds-and-the-steenrod-problem",
    "title": "Mathematics",
    "section": "Orbifolds and the Steenrod problem",
    "text": "Orbifolds and the Steenrod problem\nThe Steenrod problem was first presented in (Eilenberg 1949) and asked the following question:\n\nCan any homology class of a finite polyhedron be represented as an image of the fundamental class of some manifold?\n\nIn (Thom 1954) Thom conclusively answered this problem, completely solving it for closed orientable manifolds.\nTheorem (Thom 1954, Thm II.1) The rational homology groups of a closed orientable manifold have a basis consisting of classes represented by closed embedded submanifolds.\nFor solving this problem, and for his related work inventing cobordism theory, Thom was awarded the Fields medal in 1958. Aided by the modern language of ep-groupoids, I was able to follow the same approach as Thom and obtain the following analogue for orbifolds.\nTheorem (Schmaltz 2019a, The Steenrod problem for orbifolds) The rational homology groups of a closed orientable orbifold have a basis consisting of classes represented by “closed embedded full suborbifolds whose normal bundles have fiberwise trivial isotropy action”.\nIn other words, given a closed orientable orbifold \\(\\mathcal{O}\\) there exists a basis \\(\\{[\\mathcal{X}_i]\\}\\) of the rational homology groups \\(H_*(\\mathcal{O};\\mathbb{Q})\\) which consists of the fundamental classes of such “closed embedded full suborbifolds \\(\\mathcal{X}_i\\subset \\mathcal{O}\\) whose normal bundles have fiberwise trivial isotropy action”. Such a suborbifold is called a representing suborbifold.\nRepresenting suborbifolds are well-suited for general intersection theories. Given such a suborbifold, the underlying topological space of the normal bundle is a vector bundle over the underlying topological space of the suborbifold. In contrast, the underlying topological space of an arbitrary orbifold bundle will generally not be a vector bundle. This means it is possible to use single valued sections (as opposed to multisections) for arguments involving perturbations.\n\n\n\nA sketch of the proof of the Steenrod problem for orbifolds, Heidelberg Geometry Seminar, 28 November, 2023"
  },
  {
    "objectID": "mathematics.html#polyfold-gromovwitten-invariants-as-intersection-numbers",
    "href": "mathematics.html#polyfold-gromovwitten-invariants-as-intersection-numbers",
    "title": "Mathematics",
    "section": "Polyfold Gromov–Witten invariants as intersection numbers",
    "text": "Polyfold Gromov–Witten invariants as intersection numbers\nThe earliest interpretations of the Gromov–Witten invariants present in the literature were given in terms of counting a finite number of curves (McDuff and Salamon 2012; Ruan 1994, 1996). For example, Ruan described the Gromov–Witten invariants as a finite sum, counted with multiplicity, of nonmultiple cover \\(J\\)-spheres in \\({\\mathcal M}^*_{(A,J)}\\) which intersect representatives of given cycles in the symplectic manifold (Ruan 1996, Thm. A).\nHowever, such definitions have previously been restricted to genus zero Gromov–Witten invariants in semipositive symplectic manifolds. Observe that in the genus \\(0\\) case the Grothendieck–Knudsen spaces \\(\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{0,k}\\) are finite-dimensional manifolds. In contrast, if genus \\(g &gt;0\\) the general Deligne–Mumford spaces \\(\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\) are orbifolds. Therefore, in the genus \\(0\\) case extant methods—such as representing a homology class as a pseudocycle in a manifold or, indeed, the Steenrod problem for manifolds—were sufficient to interpret the Gromov–Witten invariants as an intersection number.\nUsing the Steenrod problem for orbifolds, I was able to prove that the polyfold Gromov–Witten invariants may equivalently be defined as an intersection number. Let \\({\\mathcal S}_{A,g,k}(p)\\) be a perturbed Gromov–Witten moduli space, and let \\({\\mathcal X}_1\\times \\cdots \\times {\\mathcal X}_k \\times {\\mathcal B}\\) be a representing suborbifold of \\(M^k \\times \\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\).\nI showed that transversality of a perturbed solution space of a polyfold with a representing submanifolds/suborbifolds may always be achieved through either of the following:\n\nThrough the perturbation of the representing suborbifold; due to the properties of the normal bundle representing suborbifolds may always be perturbed (Schmaltz 2019a, Prop. 3.9).\nAssuming the map defined on the ambient polyfold is a submersion, we may obtain transversality through choice of a suitable abstract perturbation (Schmaltz 2019a, Prop. 3.10).\n\nWhen \\(\\dim {\\mathcal S}_{A,g,k}(p) + \\dim \\left({\\mathcal X}_1 \\times\\cdots\\times {\\mathcal X}_k \\times {\\mathcal B}\\right) = \\dim (M^k \\times \\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k})\\) the intersection number is given by the signed weighted count of a finite number of points of intersection.\nTheorem (Schmaltz 2019a, Polyfold Gromov–Witten invariants as intersection numbers) The polyfold Gromov–Witten invariant may equivalently be defined as the intersection number evaluated on a basis of representing submanifolds \\({\\mathcal X}\\subset M\\) and representing suborbifolds \\({\\mathcal B}\\subset {\\mathcal O}\\):\n\\[\n    \\mathop{\\mathrm{GW}}_{A,g,k} ([{\\mathcal X}_1],\\ldots,[{\\mathcal X}_k];[{\\mathcal B}]) :=\n    \\left(ev_1\\times\\cdots\\times ev_k\\times\\pi\\right)|_{{\\mathcal S}_{A,g,k}(p)} \\cdot \\left({\\mathcal X}_1 \\times\\cdots\\times {\\mathcal X}_k \\times {\\mathcal B}\\right).\n\\]\nThe invariant does not depend on the choice of abstract perturbation, nor on the choice of representing basis.\nThus the traditional geometric interpretation of the Gromov–Witten invariants as a “count of curves” is made literal.\n\n\n\nA Gromov–Witten invariant"
  },
  {
    "objectID": "mathematics.html#a-polyfold-proof-of-the-gromovwitten-axioms",
    "href": "mathematics.html#a-polyfold-proof-of-the-gromovwitten-axioms",
    "title": "Mathematics",
    "section": "A polyfold proof of the Gromov–Witten axioms",
    "text": "A polyfold proof of the Gromov–Witten axioms\nIn 2019, I succeeded in giving a complete proof of the Gromov–Witten axioms.\nTheorem (Schmaltz 2019c, Polyfold Gromov–Witten axioms) The polyfold Gromov–Witten invariants satisfy the Gromov–Witten axioms:\nEffective axiom. If \\(\\omega(A)&lt;0\\) then \\(\\mathop{\\mathrm{GW}}_{A,g,k} = 0\\).\nGrading axiom. If \\(\\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1,\\ldots,\\alpha_k; \\beta) \\neq 0\\) then\n\\[\n    \\sum_{i=1}^k (2n - \\deg (\\alpha_i)) + (6g-6+2k - \\deg(\\beta)) = 2c_1(A) + (2n - 6)(1-g) + 2k.\n\\]\nHomology axiom. There exists a homology class\n\\[\n        \\sigma_{A,g,k} \\in H_{2c_1(A) + (2n-6)(1-g) + 2k} (M^k\\times \\bar{\\mathcal{M}}_{g,k};{\\mathbb Q})\n\\]\nsuch that\n\\[\n        \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1,\\ldots,\\alpha_k; \\beta) = \\langle p_1^* \\mathop{\\mathrm{PD}}(\\alpha_1) \\smallsmile \\cdots \\smallsmile p_k^*\\mathop{\\mathrm{PD}}(\\alpha_k) \\smallsmile p_0^*\\mathop{\\mathrm{PD}}(\\beta), \\sigma_{A,g,k} \\rangle\n\\]\nwhere \\(p_i: M^k \\times \\bar{\\mathcal{M}}_{g,k} \\to M\\) denotes the projection onto the \\(i\\)th factor and the map \\(p_0:M^k \\times \\bar{\\mathcal{M}}_{g,k}\\to\\bar{\\mathcal{M}}_{g,k}\\) denotes the projection onto the last factor.\nZero axiom. If \\(A=0,\\ g=0\\) then \\(\\mathop{\\mathrm{GW}}_{0,0,k} (\\alpha_1,\\ldots,\\alpha_k;\\beta) = 0\\) whenever \\(\\deg (\\beta) &gt;0\\), and\n\\[\n        \\mathop{\\mathrm{GW}}_{0,0,k} (\\alpha_1,\\ldots,\\alpha_k; [\\operatorname{pt}]) = \\int_M \\mathop{\\mathrm{PD}}(\\alpha_1) \\wedge \\cdots \\wedge \\mathop{\\mathrm{PD}}(\\alpha_k).\n\\]\nSymmetry axiom. Fix a permutation \\(\\sigma: \\{1,\\ldots, k\\}\\to \\{1,\\ldots,k\\}\\). Consider the permutation map \\(\\sigma:\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\to \\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}, \\ [\\Sigma,j,M,D]  \\mapsto [\\Sigma,j,M^\\sigma,D]\\) where \\(M = \\{z_1,\\ldots,z_k\\}\\) and where \\(M^\\sigma := \\{z'_1,\\ldots,z'_k\\},\\) \\(z'_i:= z_{\\sigma(i)}\\). Then\n\\[\n        \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_{\\sigma(1)},\\ldots,\\alpha_{\\sigma(k)}; \\sigma_*\\beta) = (-1)^{N(\\sigma;\\alpha_i)} \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1,\\ldots,\\alpha_k; \\beta)\n\\]\nwhere \\(N(\\sigma;\\alpha_i):= \\sharp    \\{  i&lt;j \\mid \\sigma(i)&gt; \\sigma(j), \\deg (\\alpha_i)\\deg(\\alpha_j)\\in 2{\\mathbb Z}+1\\}\\).\nDefinition. (Kontsevich and Manin 1994, Eq. 2.3) We say that \\((A,g,k)\\) is a basic class if it is equal to one of the following: \\((A,0,3)\\), \\((A,1,1)\\), or \\((A,g\\geq 2,0)\\).\nThe point is, for such values of \\(g\\) and \\(k\\) we will have \\(\\bar{\\mathcal{M}}_{g,k-1} = \\emptyset\\) by definition.\nFundamental class axiom. Consider the fundamental classes \\([M]\\in H_{2n}(M;{\\mathbb Q})\\) and \\([\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}] \\in H_{6g-6+2k}(\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k};{\\mathbb Q})\\). Suppose that \\(A\\neq 0\\) and that \\((A,g,k)\\) is not basic. Then\n\\[\n        \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1,\\ldots,\\alpha_{k-1},[M]; [\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}]) = 0.\n\\]\nConsider the canonical section \\(s_i :\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k-1} \\to \\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\) defined by doubling the \\(i\\)th-marked point. Then\n\\[\n        \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1,\\ldots,\\alpha_{k-1},[M]; s_{i*}\\beta) = \\mathop{\\mathrm{GW}}_{A,g,k-1} (\\alpha_1,\\ldots,\\alpha_{k-1};\\beta).\n\\]\nDivisor axiom. Suppose \\((A,g,k)\\) is not basic. If \\(\\deg (\\alpha_k) = 2n-2\\) then\n\\[\n        \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1,\\ldots,\\alpha_k; \\mathop{\\mathrm{PD}}(ft_k^* \\mathop{\\mathrm{PD}}(\\beta))) = (A\\cdot \\alpha_k ) \\ \\mathop{\\mathrm{GW}}_{A,g,k-1} (\\alpha_1,\\ldots,\\alpha_{k-1};\\beta),\n\\]\nwhere \\(A\\cdot \\alpha_k\\) is given by the homological intersection product.\nLet \\(\\{e_\\nu \\} \\in H^*(M;{\\mathbb Q})\\) be a homogeneous basis and let \\(\\{e^\\mu \\} \\in H^*(M;{\\mathbb Q})\\) be the dual basis with respect to Poincaré duality, i.e., \\(\\langle e_\\nu \\smallsmile e^\\mu, [M] \\rangle = \\delta_{\\nu \\mu}\\). It follows from the Künneth formula that \\(\\{e_\\nu \\otimes e^\\mu \\}\\) is a basis for \\(H^*(M\\times M;{\\mathbb Q})\\). We correct the sign by redefining \\(e_\\nu\\) as \\((-1)^{\\deg e_\\nu} e_\\nu\\). We can write the Poincaré dual of the diagonal \\(\\Delta \\subset M\\times M\\) in this basis as \\(\\mathop{\\mathrm{PD}}([\\Delta]) = \\sum_\\nu e_\\nu \\otimes e^\\nu\\).\nSplitting axiom. Fix a partition \\(S_0 \\sqcup S_1 =\\{1,\\ldots, k\\}\\). Let \\(k_0 := \\sharp S_0\\), \\(k_1 := \\sharp S_1\\) and let \\(g_0\\), \\(g_1 \\geq 0\\) such that \\(g = g_0 + g_1\\), and \\(k_i + g_i \\geq 2\\) for \\(i=0,1\\). Consider the natural map \\[\\phi_S : \\bar{\\mathcal{M}}_{k_0+1 , g_0}\\times \\bar{\\mathcal{M}}_{k_1+1 , g_1} \\to \\bar{\\mathcal{M}}_{g,k}\\] which identifies the last marked point of a stable noded Riemann surface in \\(\\bar{\\mathcal{M}}_{k_0+1 , g_0}\\) with the first marked point of a stable noded Riemann surface in \\(\\bar{\\mathcal{M}}_{k_1+1, g_1}\\), and which maps the first \\(k_0\\) marked points of \\(\\bar{\\mathcal{M}}_{g_0,k_0+1}\\) to marked points indexed by \\(S_0\\) and likewise maps the last \\(k_1\\) marked points of \\(\\bar{\\mathcal{M}}_{g_1,k_1+1}\\) to marked points indexed by \\(S_1\\). Then\n\\[\n    \\begin{align*}\n         & \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1, \\ldots, \\alpha_k; \\phi_{S*} (\\beta_0\\otimes \\beta_1) ) =\n        (-1)^{N(S;\\alpha)} \\sum_{A_0+A_1 = A}   \\sum_\\nu                                     \\\\\n         & \\qquad\n        \\mathop{\\mathrm{GW}}_{A_0,g_0,k_0+1} (\\{\\alpha_i\\}_{i\\in S_0}, \\mathop{\\mathrm{PD}}(e_\\nu) ; \\beta_0)\n        \\cdot\n        \\mathop{\\mathrm{GW}}_{A_1,g_1,k_1+1} (\\mathop{\\mathrm{PD}}(e^\\nu), \\{\\alpha_j\\}_{j\\in S_1} ; \\beta_1)\n    \\end{align*}\n\\]\nwhere \\(N(S;\\alpha)=\\sharp \\{j&lt;i \\mid i\\in S_0, j\\in S_1, \\deg(\\alpha_i)\\deg(\\alpha_j)\\in 2{\\mathbb Z}+1 \\}\\).\nGenus reduction axiom. Consider the natural map \\[\\psi: \\bar{\\mathcal{M}}_{g-1,k+2} \\to \\bar{\\mathcal{M}}_{g,k}\\] which identifies the last two marked points of a stable noded Riemann surface, increasing the arithmetic genus by one. Then\n\\[\n  2 \\cdot \\mathop{\\mathrm{GW}}_{A,g,k} (\\alpha_1, \\ldots, \\alpha_k; \\psi_* \\beta) = \\sum_\\nu \\mathop{\\mathrm{GW}}_{A,g-1,k+2} (\\alpha_1,\\ldots,\\alpha_k, \\mathop{\\mathrm{PD}}(e_\\nu) , \\mathop{\\mathrm{PD}}(e^\\nu) ; \\beta).\n\\]\n\n\n\nGromov–Witten Axioms for Symplectic Manifolds via Polyfold Theory, Symplectic Field Theory IX, 30 August 2018"
  },
  {
    "objectID": "mathematics.html#strategy-of-the-proof-of-the-gromovwitten-axioms",
    "href": "mathematics.html#strategy-of-the-proof-of-the-gromovwitten-axioms",
    "title": "Mathematics",
    "section": "Strategy of the proof of the Gromov–Witten axioms",
    "text": "Strategy of the proof of the Gromov–Witten axioms\nThe Gromov–Witten axioms give relationships between the Gromov–Witten invariants. These relationships are determined by the geometry of certain naturally defined maps defined between the unperturbed Gromov–Witten moduli spaces, namely:\n\npermutation maps, \\[\\sigma : \\bar{\\mathcal{M}}_{A,g,k}(J) \\to \\bar{\\mathcal{M}}_{A,g,k}(J),\\]\n\\(k\\)th-marked point forgetting maps, \\[ft_k : \\bar{\\mathcal{M}}_{A,g,k}(J) \\to \\bar{\\mathcal{M}}_{A,g,k-1}(J),\\]\ncanonical sections, \\[s_i : \\bar{\\mathcal{M}}_{A,g,k-1}(J) \\hookrightarrow \\bar{\\mathcal{M}}_{A,g,k}(J).\\]\n\nFurthermore, using the map\n\\[\n    ev_{k_0+1} \\times ev_1 : \\bar{\\mathcal{M}}_{A_0,g_0,k_0+1}(J) \\times \\bar{\\mathcal{M}}_{A_1,g_1,k_1+1}(J) \\to M\\times M\n\\]\nwe may consider the subset \\((ev_{k_0+1} \\times ev_1 )^{-1}(\\Delta)\\) of the product unperturbed Gromov–Witten moduli space with a constraint imposed by the diagonal \\(\\Delta \\subset M\\times M\\). We then additionally have:\n\ninclusion maps, and maps \\(\\phi\\) which identify the marked points \\(z_{k_0+1}\\) and \\(z_1'\\), \\[\n  \\begin{align*}\n      \\bar{\\mathcal{M}}_{A_0,g_0,k_0+1}(J) & \\times \\bar{\\mathcal{M}}_{A_1,g_1,k_1+1}(J) \\\\\n      i \\bigg{\\uparrow} & \\\\\n      (ev_{k_0+1} & \\times ev_1 )^{-1}(\\Delta) \\xrightarrow{\\phi} \\bar{\\mathcal{M}}_{A_0+A_1,g_0+g_1,k_0+k_1}(J)\n  \\end{align*}\n\\] Likewise, using the map \\(ev_{k+1} \\times ev_{k+2} : \\bar{\\mathcal{M}}_{A,g-1,k+2}(J) \\to M\\times M\\) we may consider the subset \\((ev_{k+1} \\times ev_{k+2} )^{-1}(\\Delta)\\) of the unperturbed Gromov–Witten moduli space with a constraint imposed by the diagonal \\(\\Delta \\subset M\\times M\\). We then additionally have:\ninclusion maps, and maps \\(\\psi\\) which identify the marked points \\(z_{k+1}\\) and \\(z_{k+2}\\) (increasing the arithmetic genus by one), \\[\n  \\begin{align*}\n      \\bar{\\mathcal{M}}_{A,g-1,k+2}& (J)    \\\\\n      i \\bigg{\\uparrow} \\qquad &\\\\\n      (ev_{k+1} \\times ev_{k+2}& )^{-1}(\\Delta) \\xrightarrow{\\psi} \\bar{\\mathcal{M}}_{A,g,k}(J)\n  \\end{align*}\n\\]\n\nIntuitively, we should prove the Gromov–Witten axioms by interpreting the Gromov–Witten invariants as a finite count of curves and using the geometry of the above maps to directly compare such counts with respect to constraints imposed by the homology classes on \\(M\\) and \\(\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\).\nA substantial amount of work is required to make this intuition rigorous in the context of an abstract perturbation theory. A deep understanding of the full machinery of polyfold theory, in addition to the geometry of the Gromov–Witten invariants is necessary to navigate substantial difficulties that we encounter."
  },
  {
    "objectID": "mathematics.html#difficulties-in-proving-the-polyfold-gromovwitten-axioms",
    "href": "mathematics.html#difficulties-in-proving-the-polyfold-gromovwitten-axioms",
    "title": "Mathematics",
    "section": "Difficulties in proving the polyfold Gromov–Witten axioms",
    "text": "Difficulties in proving the polyfold Gromov–Witten axioms\nProving this required a substantial amount of work, and relied on the results of (Schmaltz 2019b, 2019a). The Gromov–Witten axioms give relationships between the Gromov–Witten invariants. These relationships are determined by the geometry of certain naturally defined maps defined between the unperturbed Gromov–Witten moduli spaces, namely the permutation maps, the \\(k\\)th-marked point forgetting maps, in addition to certain natural maps which identify marked points into nodal pairs, where the Gromov–Witten moduli space is subject to a diagonal constraint. With the exception of the \\(k\\)th-marked point forgetting maps, we may pullback abstract perturbations in order to obtain well-defined restricted maps between perturbed Gromov–Witten moduli spaces.\nThe branched integral is useful for giving a well-defined definition of the polyfold Gromov–Witten invariants and moreover showing that they are, in fact, invariants and do not depend on choices. But integration is not the best viewpoint for giving a proof of all of the axioms. Intuitively, we should prove the Gromov–Witten axioms by interpreting the Gromov–Witten invariants as an intersection number and using the geometry of the above maps to directly compare such counts with respect to constraints imposed by the homology classes on \\(M\\) and \\(\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\). And indeed, via (Schmaltz 2019a) this intuition is made rigorous, and is precisely my approach to proving the axioms.\nHowever, problems arise when we try to define a \\(k\\)th-marked point forgetting map between perturbed Gromov–Witten moduli spaces—such a map does not even exist. The construction of the smooth structure for the Deligne–Mumford orbifolds as described in (Hofer, Wysocki, and Zehnder 2017a) and (Hofer, Wysocki, and Zehnder, n.d.) requires a choice: that of a “gluing profile,” i.e., a smooth diffeomorphism \\(\\varphi: (0,1]\\to [0,\\infty).\\) Given a noded Riemann surface and a nonzero parameter \\(a \\in {\\mathbb C}\\) we use the gluing profile to replace a region of the node with a cylinder of finite length \\(\\varphi(\\lvert a\\rvert)\\). The logarithmic gluing profile is given by \\(\\varphi_{\\log} (r) = -\\frac{1}{2\\pi} \\log (r)\\) and produces the classical holomorphic Deligne–Mumford orbifolds \\(\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\). There is also an exponential gluing profile, given by \\(\\varphi_{\\exp} (r) = e^{1/r} - e\\) which produces Deligne–Mumford orbifolds \\(\\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{exp}}_{g,k}\\) which are only smooth orbifolds. The exponential gluing profile is required for the scale smoothness of certain maps used to define the Gromov–Witten polyfolds.\nThis use of nonstandard smooth structure has the following consequence:\n\nProblem 1. In general the map \\(ft_k: \\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{exp}}_{g,k} \\to \\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{exp}}_{g,k-1}\\) is continuous but not differentiable.\n\nIndependent of the usage of a nonstandard gluing profile, there is no hope whatsoever of defining a \\(k\\)th-marked point forgetting map on the Gromov–Witten polyfolds as they are defined:\n\nProblem 2. In general there does not exist a natural map \\(ft_k\\) on the Gromov–Witten polyfolds.\n\nTo explain, a stable curve in \\({\\mathcal Z}_{A,g,k}\\) may contain a “destabilizing ghost component,” i.e., a component \\(C_k\\simeq S^2\\) with precisely \\(3\\) special points, one of which is the \\(k\\)th-marked point, and such that \\(\\int_{C_k} u^*\\omega=0,\\) \\(u|_{C_k} \\neq\\text{const}.\\) After removal of the \\(k\\)th-marked point from such a component we cannot consider the resulting data as a stable curve in \\({\\mathcal Z}_{A,g,k-1}\\).\nWe might try to restrict to a subset \\({\\mathcal Z}^\\text{const}_{A,g,k}\\subset {\\mathcal Z}_{A,g,k}\\) consisting of stable curves which are constant on such destabilizing ghost components. The \\(k\\)th-marked point forgetting map is then well-defined on this subset. However, if we consider \\(\\mathcal{Z}^\\text{const}_{A,g,k}\\subset \\mathcal{Z}_{A,g,k}\\) with the subspace topology, and \\(\\mathcal{Z}_{A,g,k-1}\\) with the usual polyfold topology, then:\n\nProblem 3. In general the well-defined restriction \\(ft_k:{\\mathcal Z}_{A,g,k}^\\text{const} \\to \\mathcal{Z}_{A,g,k-1}\\) is not continuous.\n\nThere is a final problem. In general, the projection map must factor through the \\(k\\)th-marked point forgetting map; this is due to the need to forget the added stabilizing points. Thus, in order to obtain a smooth projection map we must map to the logarithmic Deligne–Mumford orbifold. However:\n\nProblem 4. While the projection \\(\\pi : {\\mathcal Z}_{A,g,k} \\to \\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\) is \\(\\text{sc}\\)-smooth, in general it is not a submersion.\n\nThis has important consequences if we wish to consider the Gromov–Witten invariant as an intersection number; the only way to get transversality of the projection map with a representing suborbifold \\({\\mathcal B}\\subset \\smash{\\bar{\\mathcal{M}}}\\vphantom{\\mathcal{M}}^{\\text{log}}_{g,k}\\) is through perturbation of the suborbifold. This is made possible by the existence of representing suborbifolds, for which perturbation is possible, the existence of which is guaranteed by (Schmaltz 2019a)."
  },
  {
    "objectID": "mathematics.html#the-universal-curve-gromovwitten-polyfold",
    "href": "mathematics.html#the-universal-curve-gromovwitten-polyfold",
    "title": "Mathematics",
    "section": "The universal curve Gromov–Witten polyfold",
    "text": "The universal curve Gromov–Witten polyfold\nIn essence, the central problem is that the Gromov–Witten polyfolds as constructed are not “universal curves”. My proof of the Gromov–Witten axioms rectifies this by constructing a universal curve polyfold \\({\\mathcal Z}^\\text{uc}_{A,g,k}\\) over \\({\\mathcal Z}_{A,g,k-1}\\), on which we may consider a well-defined \\(k\\)th-marked point forgetting map\n\\[\n    ft_k : {\\mathcal Z}^\\text{uc}_{A,g,k} \\to {\\mathcal Z}_{A,g,k-1}.\n\\]\nThe preimage of stable curve in \\({\\mathcal Z}_{A,g,k-1}\\) via \\(ft_k\\) can be identified with the underlying Riemann surface with nodes identified, thereby explaining the choice of nomenclature “universal curve”. It is possible to pullback regular perturbations via this map, and hence obtain a well-defined map between perturbed Gromov–Witten moduli spaces.\nNow we find ourselves in the following situation: given the Gromov–Witten moduli space \\(\\bar{\\mathcal{M}}_{A,g,k}\\) we can define polyfold Gromov–Witten invariants associated to the usual Gromov–Witten polyfold \\({\\mathcal Z}_{A,g,k}\\) and associated to the universal curve polyfold \\({\\mathcal Z}^\\text{uc}_{A,g,k}\\) which, a priori, we cannot assume are equivalent.\nIn (Schmaltz 2019b), I present a general framework for proving that polyfold invariants are natural, which applied to polyfold Gromov–Witten theory yields the following result.\nTheorem. (Schmaltz 2019b, Naturality of the polyfold Gromov–Witten invariants) The polyfold Gromov–Witten invariants are natural, and do not depend on auxiliary choices made in their construction. In particular, the Gromov–Witten invariants associated to the usual Gromov–Witten polyfold and associated to the universal curve Gromov–Witten polyfold are identical."
  },
  {
    "objectID": "mathematics.html#pseudocycle-gromovwitten-invariants-are-a-strict-subset-of-polyfold-gromovwitten-invariants",
    "href": "mathematics.html#pseudocycle-gromovwitten-invariants-are-a-strict-subset-of-polyfold-gromovwitten-invariants",
    "title": "Mathematics",
    "section": "Pseudocycle Gromov–Witten invariants are a strict subset of polyfold Gromov–Witten invariants",
    "text": "Pseudocycle Gromov–Witten invariants are a strict subset of polyfold Gromov–Witten invariants\nI unified the classical definition of a Gromov–Witten invariant as a pseudocycle and the modern definition of a Gromov–Witten invariant via polyfold theory by proving they are equivalent.\nTheorem. (Schmaltz 2023, Main result) For a given semipositive symplectic manifold, the pseudocycle genus-zero Gromov–Witten invariants are equal to the polyfold genus-zero Gromov–Witten invariants.\nSince the polyfold Gromov–Witten invariants are not restricted to genus-zero, nor to semipositive symplectic manifolds, we have\n\\[\n    \\left\\{ \\begin{array}{c} \\textit{pseudocycle} \\\\ \\textit{Gromov-Witten} \\\\ \\textit{invariants} \\end{array} \\right\\}\n    \\subsetneq\n    \\left\\{ \\begin{array}{c} \\textit{polyfold} \\\\ \\textit{Gromov-Witten} \\\\ \\textit{invariants} \\end{array} \\right\\}.\n\\]\n\nReferences\n\n\nCastellano, Robert. 2016. Kuranishi Atlases and Genus Zero Gromov–Witten Invariants. ProQuest LLC, Ann Arbor, MI. http://gateway.proquest.com/openurl?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx:dissertation&res_dat=xri:pqm&rft_dat=xri:pqdiss:10096865 .\n\n\nCieliebak, Kai, and Klaus Mohnke. 2007. “Symplectic Hypersurfaces and Transversality in Gromov–Witten Theory.” J. Symplectic Geom. 5 (3): 281–356. http://projecteuclid.org/euclid.jsg/1210083200.\n\n\nEilenberg, Samuel. 1949. “On the Problems of Topology.” Ann. Of Math. (2) 50: 247–60. https://doi.org/10.2307/1969448.\n\n\nFabert, Oliver, Joel W. Fish, Roman Golovko, and Katrin Wehrheim. 2016. “Polyfolds: A First and Second Look.” EMS Surv. Math. Sci. 3 (2): 131–208. https://doi.org/10.4171/EMSS/16.\n\n\nFloer, Andreas. 1988. “An Instanton-Invariant for \\(3\\)-Manifolds.” Comm. Math. Phys. 118 (2): 215–40. http://projecteuclid.org/euclid.cmp/1104161987.\n\n\nFukaya, Kenji, Yong-Geun Oh, Hiroshi Ohta, and Kaoru Ono. 2012. “Technical details on Kuranishi structure and virtual fundamental chain.” arXiv e-Prints. September 2012. https://arxiv.org/abs/1209.4410.\n\n\nFukaya, Kenji, and Kaoru Ono. 1999. “Arnold Conjecture and Gromov–Witten Invariant for General Symplectic Manifolds.” In The Arnoldfest (Toronto, ON, 1997), 24:173–90. Fields Inst. Commun. Amer. Math. Soc., Providence, RI.\n\n\nGromov, M. 1985. “Pseudo Holomorphic Curves in Symplectic Manifolds.” Invent. Math. 82 (2): 307–47. https://doi.org/10.1007/BF01388806.\n\n\nHofer, Helmut, Kris Wysocki, and Eduard Zehnder. 2007. “A General Fredholm Theory. I. A Splicing-Based Differential Geometry.” J. Eur. Math. Soc. (JEMS) 9 (4): 841–76. https://doi.org/10.4171/JEMS/99.\n\n\n———. 2009a. “A General Fredholm Theory. II. Implicit Function Theorems.” Geom. Funct. Anal. 19 (1): 206–93. https://doi.org/10.1007/s00039-009-0715-x.\n\n\n———. 2009b. “A General Fredholm Theory. III. Fredholm Functors and Polyfolds.” Geom. Topol. 13 (4): 2279–2387. https://doi.org/10.2140/gt.2009.13.2279.\n\n\n———. 2010a. “Integration Theory on the Zero Sets of Polyfold Fredholm Sections.” Math. Ann. 346 (1): 139–98. https://doi.org/10.1007/s00208-009-0393-x.\n\n\n———. 2010b. “Sc-Smoothness, Retractions and New Models for Smooth Spaces.” Discrete Contin. Dyn. Syst. 28 (2): 665–788. https://doi.org/10.3934/dcds.2010.28.665.\n\n\n———. 2017a. “Applications of Polyfold Theory I: The Polyfolds of Gromov– Witten Theory.” Mem. Amer. Math. Soc. 248 (1179): v+218. https://doi.org/10.1090/memo/1179.\n\n\n———. 2017b. “Polyfold and Fredholm theory.” arXiv e-Prints. July 2017. https://arxiv.org/abs/1707.08941.\n\n\n———. n.d. “Deligne–Mumford-Type Spaces with a View Towards Symplectic Field Theory.”\n\n\nIonel, Eleny-Nicoleta, and Thomas H. Parker. 2013. “A natural Gromov–Witten virtual fundamental class.” arXiv e-Prints. February 2013. https://arxiv.org/abs/1302.3472.\n\n\nKontsevich, Maxim. 1995. “Enumeration of Rational Curves via Torus Actions.” In The Moduli Space of Curves (Texel Island, 1994), 129:335–68. Progr. Math. Birkhäuser Boston, Boston, MA. https://doi.org/10.1007/978-1-4612-4264-2_12.\n\n\nKontsevich, Maxim, and Yuri Manin. 1994. “Gromov–Witten Classes, Quantum Cohomology, and Enumerative Geometry.” Comm. Math. Phys. 164 (3): 525–62. http://projecteuclid.org/euclid.cmp/1104270948.\n\n\nLi, Jun, and Gang Tian. 1998. “Virtual Moduli Cycles and Gromov–Witten Invariants of General Symplectic Manifolds.” In Topics in Symplectic \\(4\\)-Manifolds (Irvine, CA, 1996), 47–83. First Int. Press Lect. Ser., i. Int. Press, Cambridge, MA.\n\n\nMcDuff, Dusa. 1991. “Symplectic Manifolds with Contact Type Boundaries.” Invent. Math. 103 (3): 651–71. https://doi.org/10.1007/BF01239530.\n\n\nMcDuff, Dusa, and Dietmar Salamon. 2012. \\(J\\)-Holomorphic Curves and Symplectic Topology. Second. Vol. 52. American Mathematical Society Colloquium Publications. American Mathematical Society, Providence, RI.\n\n\nMcDuff, Dusa, and Katrin Wehrheim. 2012. “Kuranishi atlases with trivial isotropy - the 2013 state of affairs.” arXiv e-Prints. August 2012. https://arxiv.org/abs/1208.1340.\n\n\n———. 2017. “The Topology of Kuranishi Atlases.” Proc. Lond. Math. Soc. (3) 115 (2): 221–92. https://doi.org/10.1112/plms.12032.\n\n\n———. 2018. “The Fundamental Class of Smooth Kuranishi Atlases with Trivial Isotropy.” J. Topol. Anal. 10 (1): 71–243. https://doi.org/10.1142/S1793525318500048.\n\n\nPardon, John. 2016. “An Algebraic Approach to Virtual Fundamental Cycles on Moduli Spaces of Pseudo-Holomorphic Curves.” Geom. Topol. 20 (2): 779–1034. https://doi.org/10.2140/gt.2016.20.779.\n\n\nRuan, Yongbin. 1994. “Symplectic Topology on Algebraic \\(3\\)-Folds.” J. Differential Geom. 39 (1): 215–27. http://projecteuclid.org/euclid.jdg/1214454682.\n\n\n———. 1996. “Topological Sigma Model and Donaldson-Type Invariants in Gromov Theory.” Duke Math. J. 83 (2): 461–500. https://doi.org/10.1215/S0012-7094-96-08316-7.\n\n\nRuan, Yongbin, and Gang Tian. 1995. “A Mathematical Theory of Quantum Cohomology.” J. Differential Geom. 42 (2): 259–367. http://projecteuclid.org/euclid.jdg/1214457234.\n\n\nSchmaltz, Wolfgang. 2019a. “The Steenrod problem for orbifolds and polyfold invariants as intersection numbers.” arXiv e-Prints. April 2019. https://arxiv.org/abs/1904.02186.\n\n\n———. 2019b. “Naturality of polyfold invariants and pulling back abstract perturbations.” arXiv e-Prints. December 2019. https://arxiv.org/abs/1912.13370.\n\n\n———. 2019c. “The Gromov–Witten axioms for symplectic manifolds via polyfold theory.” arXiv e-Prints. December 2019. https://arxiv.org/abs/1912.13374.\n\n\n———. 2023. “Pseudocycle Gromov-Witten invariants are a strict subset of polyfold Gromov-Witten invariants.” arXiv e-Prints. https://doi.org/10.48550/arXiv.2308.14204.\n\n\nSiebert, Bernd. 1996. “Gromov–Witten invariants of general symplectic manifolds.” Eprint arXiv:dg-Ga/960800. August 1996. https://arxiv.org/abs/dg-ga/9608005.\n\n\nThom, René. 1954. “Quelques Propriétés Globales Des Variétés Diffé Rentiables.” Comment. Math. Helv. 28: 17–86. https://doi.org/10.1007/BF02566923.\n\n\nWitten, Edward. 1988. “Topological Sigma Models.” Comm. Math. Phys. 118 (3): 411–49. http://projecteuclid.org/euclid.cmp/1104162092.\n\n\n———. 1991. “Two-Dimensional Gravity and Intersection Theory on Moduli Space.” In Surveys in Differential Geometry (Cambridge, MA, 1990), 243–310. Lehigh Univ., Bethlehem, PA."
  }
]